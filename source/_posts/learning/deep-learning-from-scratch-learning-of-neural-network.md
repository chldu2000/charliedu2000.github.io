---
title: 深度学习入门：神经网络的学习
tags:
  - 神经网络
  - 深度学习
categories:
  - 快去学习
date: 2022-10-07 23:18:35
katex: true
---

**施工中……**

内容参考：

- 《深度学习入门：基于 Python 的理论与实现》(斋藤康毅)
- [上述书籍作者提供的代码](https://github.com/oreilly-japan/deep-learning-from-scratch)

之前实现了神经网络的前向传播过程，但是用到的权重参数是预先准备好的，那么接下来的目标自然就是“学习”——从训练数据中自动获取最优权重参数。

## 从数据中学习

机器学习是以数据为核心的，尝试从数据中发现答案和模式。*神经网络或深度学习则比以往的机器学习方法更能避免人为介入*。

要理解上面这些描述还是要从具体的问题开始，比如要实现数字“5”的识别该用什么算法呢？人很容易认出来眼前的是不是5，这可以说是基于经验和某些规律得出的结果，但是机器怎么知道这些经验或者规律是什么呢？与其想办法用算法表示这些规律（显然很难），不如想办法让机器从数据中直接获取需要的“规律”。现有的一种方案是先从图像中提取**特征量**，再用机器学习来学习这些特征量的模式。

> 这里所说的“特征量”是指可以从输入数据（输入图像）中准确地提取本质数据（重要的数据）的转换器。图像的特征量通常表示为向量的形式。在计算机视觉领域，常用的特征量包括SIFT、SURF和HOG等。使用这些特征量将图像数据转换为向量，然后对转换后的向量使用机器学习中的SVM、KNN等分类器进行学习。

在上面所说的方法中，将图像转换为向量时所用的特征量仍是人为设计的，对于不同的问题需要考虑不同的特征量。在深度学习中，这里所说的特征量也是由机器来学习的。

![从人工设计规则转变为由机器从数据中学习](https://s2.loli.net/2022/10/08/jr6XuH83fihJRdU.png)

书上给出的这幅图说明了识别的方法，从这里也能看出所谓“机器学习”和“深度学习”之间的一些区别。

> 深度学习有时也称为端到端机器学习（end-to-end machine learning）。这里所说的端到端是指从一端到另一端的意思，也就是从原始数据（输入）中获得目标结果（输出）的意思。

~~就是说中间的过程不需要人为干预吗……~~

### 一些注意事项： 训练数据和测试数据

很明显，**训练数据**就是用来学习、寻找最优参数所用的那部分数据，而**测试数据**（监督数据）的作用是评价模型的泛化能力，自然不可缺少。如果只用已有的数据来学习和评价，可能会造成模型无法正确处理其他数据集，也就是出现**过拟合**现象。

### 损失函数

**损失函数**可以表示神经网络的“状态”，神经网络就参考这个状态指标来寻找最优权重参数。这里说的损失函数表示的“状态”是指当前神经网络神经网络对监督数据在多大程度上不拟合、不一致，或者说性能有多坏（当然，为损失函数乘上一个负值就可以表示性能有多好）。损失函数一般用均方误差和交叉熵误差等。

#### 均方误差

均方误差的公式如下：

$$ E = \frac{1}{2} \sum \limits_k (y_k - t_k)^2 $$

其中 $y_k$ 表示神经网络的输出，$t_k$ 表示监督数据，$k$ 表示数据的维数。比方说在之前的手写数字识别的例子中，$y_k$、$t_k$ 就是由10个元素构成的数据：

```pycon
>>> y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
>>> t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
```

`y` 是 `softmax` 函数的输出，`t` 是监督数据的独热编码形式，均方误差会计算 `y` 和 `t` 的各个对应元素之差的平方，再求总和。实现方式可以是：

```python
# 均方误差
def mean_squared_error(y, t):
    return 0.5 * np.sum((y - t) ** 2)
```

均方误差得出的结果越小，说明结果与监督数据之间的误差越小。

#### 交叉熵误差

公式如下：

$$ E = - \sum \limits_k t_k \log y_k $$

其中 $\log$ 表示 $\log_e$，$y_k$ 是神经网络的输出，$t_k$ 是正确解的标签（独热编码）。显然上式只会计算正确解标签的输出的自然对数（因为只有正确解标签的索引为1），也就是说交叉熵误差的值是由正确解标签所对应的输出结果决定的。

自然对数 $y = \log x$ 的图像如下：

![自然对数 y = log x 的图像](https://s2.loli.net/2022/10/09/3dHoWFkPCrpvBGh.png)

那么可以看出正确解标签对应的输出为1时，交叉熵误差的输出为0，输出越小，交叉熵误差的结果越大。

实现方式：

```python
# 交叉熵误差
def cross_entropy_error(y, t):
    delta = 1e-7
    return -np.sum(t * np.log(y + delta))  # 防止出现 log(0) 无限大
```

计算例子：

```pycon
>>> t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
>>> y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
>>> cross_entropy_error(np.array(y), np.array(t))
0.51082545709933802
>>>
>>> y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
>>> cross_entropy_error(np.array(y), np.array(t))
2.3025840929945458
```

#### mini-batch 学习

所谓使用训练数据进行学习，就是针对训练数据计算损失函数的值，找出使该值尽可能小的参数。计算损失函数时必须将所有的训练数据作为对象，考虑所有训练数据的损失函数的总和。

以交叉熵误差为例，如果要计算所有训练数据的损失函数的总和，可以这样：

$$ E = - \frac{1}{N} \sum \limits_n \sum \limits_k t_{nk} \log y_{nk} $$

N 自然是数据个数，$y_{nk}$ 是神经网络的输出中第 n 个数据的第 k 个元素的值，$t_{nk}$ 则是对应的监督数据。观察可以发现这里其实就是把单个数据的损失函数的公式扩大到了 N 个数据，随后除以 N 正规化处理。这个式子也可以说是“平均损失函数”。

在数据集比较大的情况下（比如之前用到的 MNIST 数据集，训练数据就有60000个），根据所有的训练数据来算损失函数的和代价就太大了。实际操作中一般是从全部数据中选出一部分作为全部数据的“近似”（**mini-batch**），用这些数据进行学习（mini-batch 学习）。比方说从60000个训练数据中随机选择100笔，在用这100笔数据进行学习。

（按我的理解，书上这里的说法应该是用样本代表整体。搜到的比较多的说法是将一批所有数据再分成若干份，称作 mini-batch，训练时一次更新一个 mini-batch，整个数据集会更新多次，最终还是使用了所有数据。）

随机抽取数据时可以利用 `np.random.choice()`，像这样：

```python
train_size = x_train.shape[0]  # 对于 MNIST 的训练数据，是60000
batch_size = 10
batch_mask = np.random.choice(train_size, batch_size)  # 在0到59999之间随机选择10个数，得到包含被选数据的索引的数组
x_batch = x_train[batch_mask]  # 被选测试数据
t_batch = t_train[batch_mask]  # 被选测试数据的标签
```

如果要实现 mini-batch 的交叉熵误差，可以这样：

```python
def cross_entropy_error(y, t):
    # 只有一个数据的时候改变数据形状
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)
 
    batch_size = y.shape[0]
    return -np.sum(t * np.log(y + 1e-7)) / batch_size
```

