---
title: 深度学习入门：神经网络的学习
tags:
  - 神经网络
  - 深度学习
categories:
  - 快去学习
date: 2022-10-07 23:18:35
katex: true
---

**施工中……**

内容参考：

- 《深度学习入门：基于 Python 的理论与实现》(斋藤康毅)
- [上述书籍作者提供的代码](https://github.com/oreilly-japan/deep-learning-from-scratch)

之前实现了神经网络的前向传播过程，但是用到的权重参数是预先准备好的，那么接下来的目标自然就是“学习”——从训练数据中自动获取最优权重参数。

## 从数据中学习

机器学习是以数据为核心的，尝试从数据中发现答案和模式。*神经网络或深度学习则比以往的机器学习方法更能避免人为介入*。

要理解上面这些描述还是要从具体的问题开始，比如要实现数字“5”的识别该用什么算法呢？人很容易认出来眼前的是不是5，这可以说是基于经验和某些规律得出的结果，但是机器怎么知道这些经验或者规律是什么呢？与其想办法用算法表示这些规律（显然很难），不如想办法让机器从数据中直接获取需要的“规律”。现有的一种方案是先从图像中提取**特征量**，再用机器学习来学习这些特征量的模式。

> 这里所说的“特征量”是指可以从输入数据（输入图像）中准确地提取本质数据（重要的数据）的转换器。图像的特征量通常表示为向量的形式。在计算机视觉领域，常用的特征量包括SIFT、SURF和HOG等。使用这些特征量将图像数据转换为向量，然后对转换后的向量使用机器学习中的SVM、KNN等分类器进行学习。

在上面所说的方法中，将图像转换为向量时所用的特征量仍是人为设计的，对于不同的问题需要考虑不同的特征量。在深度学习中，这里所说的特征量也是由机器来学习的。

![从人工设计规则转变为由机器从数据中学习](https://s2.loli.net/2022/10/08/jr6XuH83fihJRdU.png)

书上给出的这幅图说明了识别的方法，从这里也能看出所谓“机器学习”和“深度学习”之间的一些区别。

> 深度学习有时也称为端到端机器学习（end-to-end machine learning）。这里所说的端到端是指从一端到另一端的意思，也就是从原始数据（输入）中获得目标结果（输出）的意思。

~~就是说中间的过程不需要人为干预吗……~~

### 一些注意事项： 训练数据和测试数据

很明显，**训练数据**就是用来学习、寻找最优参数所用的那部分数据，而**测试数据**（监督数据）的作用是评价模型的泛化能力，自然不可缺少。如果只用已有的数据来学习和评价，可能会造成模型无法正确处理其他数据集，也就是出现**过拟合**现象。

### 损失函数

**损失函数**可以表示神经网络的“状态”，神经网络就参考这个状态指标来寻找最优权重参数。这里说的损失函数表示的“状态”是指当前神经网络神经网络对监督数据在多大程度上不拟合、不一致，或者说性能有多坏（当然，为损失函数乘上一个负值就可以表示性能有多好）。损失函数一般用均方误差和交叉熵误差等。

#### 均方误差

均方误差的公式如下：

$$ E = \frac{1}{2} \sum \limits_k (y_k - t_k)^2 $$

其中 $y_k$ 表示神经网络的输出，$t_k$ 表示监督数据，$k$ 表示数据的维数。比方说在之前的手写数字识别的例子中，$y_k$、$t_k$ 就是由10个元素构成的数据：

```pycon
>>> y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
>>> t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
```

`y` 是 `softmax` 函数的输出，`t` 是监督数据的独热编码形式，均方误差会计算 `y` 和 `t` 的各个对应元素之差的平方，再求总和。实现方式可以是：

```python
# 均方误差
def mean_squared_error(y, t):
    return 0.5 * np.sum((y - t) ** 2)
```

均方误差得出的结果越小，说明结果与监督数据之间的误差越小。

#### 交叉熵误差

公式如下：

$$ E = - \sum \limits_k t_k \log y_k $$
