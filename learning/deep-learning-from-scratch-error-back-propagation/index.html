<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="零歌"><meta name="copyright" content="零歌"><meta name="generator" content="Hexo 6.3.0"><meta name="theme" content="hexo-theme-yun"><title>深度学习入门：误差反向传播法 | 愚人而已</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/star-markdown-css@0.4.1/dist/yun/yun-markdown.min.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/prism-theme-vars/base.css"><script src="https://fastly.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>function initScrollReveal() {
  [".post-card",".markdown-body img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
}
document.addEventListener("DOMContentLoaded", initScrollReveal);
document.addEventListener("pjax:success", initScrollReveal);
</script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"></script><script type="module">import { renderKatex } from '/js/utils.js'
document.addEventListener("DOMContentLoaded", () => {
  renderKatex({
    ...{},
    ...true?.options,
  });
});</script><link rel="icon" type="image/svg+xml" href="/afool.svg"><link rel="mask-icon" href="/afool.svg" color="#0078E7"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="preconnect" href="https://fastly.jsdelivr.net/npm/" crossorigin><script id="yun-config">
    window.Yun = {}
    window.CONFIG = {"hostname":"afool.top","root":"/","title":"愚人而已","version":"1.10.9","mode":"auto","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"搜索...","empty":"找不到您查询的内容: ${query}","hits":"找到 ${hits} 条结果","hits_time":"找到 ${hits} 条结果（用时 ${time} 毫秒）"},"anonymous_image":"https://cdn.yunyoujun.cn/img/avatar/none.jpg","say":{"api":"https://el-bot-api.vercel.app/api/words/young"},"fireworks":{"colors":null},"vendors":{"host":"https://fastly.jsdelivr.net/npm/","darken":"https://fastly.jsdelivr.net/npm/darken@1.5.0"}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/hexo-theme-yun.js" type="module"></script><link rel="alternate" href="/atom.xml" title="愚人而已" type="application/atom+xml"><meta name="description" content="内容参考：  《深度学习入门：基于 Python 的理论与实现》(斋藤康毅) 上述书籍作者提供的代码  之前成功实现了一个神经网络的学习过程，但是实际开始计算就会发现整个过程实在是太慢了……对所有参数都要计算梯度，而且要计算很多遍，显然我们需要一种能更快地计算出权重参数梯度的方法。 关于计算图计算图能够将计算过程用图形（数据结构图，用多个节点和边表示）表示出来。 计算图与反向传播使用计算图是为了直">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习入门：误差反向传播法">
<meta property="og:url" content="https://afool.top/learning/deep-learning-from-scratch-error-back-propagation/">
<meta property="og:site_name" content="愚人而已">
<meta property="og:description" content="内容参考：  《深度学习入门：基于 Python 的理论与实现》(斋藤康毅) 上述书籍作者提供的代码  之前成功实现了一个神经网络的学习过程，但是实际开始计算就会发现整个过程实在是太慢了……对所有参数都要计算梯度，而且要计算很多遍，显然我们需要一种能更快地计算出权重参数梯度的方法。 关于计算图计算图能够将计算过程用图形（数据结构图，用多个节点和边表示）表示出来。 计算图与反向传播使用计算图是为了直">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2022/10/18/4hJ7QlSEdDjKzn3.png">
<meta property="og:image" content="https://s2.loli.net/2022/10/18/iW1Ge4oqmv9Pb27.png">
<meta property="og:image" content="https://s2.loli.net/2022/10/20/RlpfhqoJxEeiCaH.png">
<meta property="og:image" content="https://s2.loli.net/2022/10/20/QnoPifB5gbxHaW1.png">
<meta property="og:image" content="https://s2.loli.net/2022/10/20/a6QUWR3ypTjJqDm.png">
<meta property="og:image" content="https://s2.loli.net/2022/10/20/EM6uPf52a3kHmCc.png">
<meta property="og:image" content="https://s2.loli.net/2022/10/20/5Qo7dfV9tECyL1M.png">
<meta property="og:image" content="https://s2.loli.net/2022/10/20/IJtF3jzyrCfnOQ9.png">
<meta property="og:image" content="https://s2.loli.net/2022/10/21/KieElhcZmUMOJCQ.png">
<meta property="og:image" content="https://s2.loli.net/2022/10/21/LCrvOc4AmQhdpgz.png">
<meta property="og:image" content="https://s2.loli.net/2022/10/21/TZjQGdIFMoLEUvO.png">
<meta property="og:image" content="https://s2.loli.net/2022/10/21/Zwpf3F4E8xglh1u.png">
<meta property="og:image" content="https://s2.loli.net/2022/10/21/f2YUTkF5xceru6M.png">
<meta property="og:image" content="https://s2.loli.net/2022/10/23/kD8buZ2w9QaBOor.png">
<meta property="og:image" content="https://s2.loli.net/2022/10/23/ngJBVy6RGq2o4x3.png">
<meta property="og:image" content="https://s2.loli.net/2022/10/28/wgac9kBEe8UbGQs.png">
<meta property="og:image" content="https://s2.loli.net/2022/10/28/F3Yjm1lEyh2nNVP.png">
<meta property="og:image" content="https://s2.loli.net/2022/10/30/cjdiZV8CTMteoYR.png">
<meta property="article:published_time" content="2022-10-17T15:06:12.000Z">
<meta property="article:modified_time" content="2022-11-09T12:40:26.324Z">
<meta property="article:author" content="零歌">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2022/10/18/4hJ7QlSEdDjKzn3.png"><script>(function() {
  if (CONFIG.mode !== 'auto') return
  const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches
  const setting = localStorage.getItem('darken-mode') || 'auto'
  if (setting === 'dark' || (prefersDark && setting !== 'light'))
    document.documentElement.classList.toggle('dark', true)
})()</script></head><body><script src="https://code.iconify.design/2/2.1.1/iconify.min.js"></script><script>// Define global variable
IconifyProviders = {
  // Empty prefix: overwrite default API provider configuration
  '': {
    // Use custom API first, use Iconify public API as backup
    resources: [
        'https://api.iconify.design',
    ],
    // Wait for 1 second before switching API hosts
    rotate: 1000,
  },
};</script><script defer src="https://fastly.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js" type="module"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js" type="module"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="文章目录"><span class="icon iconify" data-icon="ri:list-ordered"></span></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><span class="icon iconify" data-icon="ri:passport-line"></span></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="零歌"><img width="96" loading="lazy" src="/images/nekosense.jpeg" alt="零歌"></a><div class="site-author-name"><a href="/about/">零歌</a></div><span class="site-name">愚人而已</span><sub class="site-subtitle">以愚者之名攀上顶峰</sub><div class="site-description"></div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="首页"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:home-4-line"></span></span></a><div class="site-state-item"><a href="/archives/" title="归档"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:archive-line"></span></span><span class="site-state-item-count">45</span></a></div><div class="site-state-item"><a href="/categories/" title="分类"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:folder-2-line"></span></span><span class="site-state-item-count">5</span></a></div><div class="site-state-item"><a href="/tags/" title="标签"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="site-state-item-count">33</span></a></div><a class="site-state-item hty-icon-button" target="_blank" rel="noopener" href="https://yun.yunyoujun.cn" title="文档"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:settings-line"></span></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="/atom.xml" title="RSS" target="_blank" style="color:orange"><span class="icon iconify" data-icon="ri:rss-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/charliedu2000" title="GitHub" target="_blank" style="color:#6e5494"><span class="icon iconify" data-icon="ri:github-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://music.163.com/#/user/home?id=610609902" title="网易云音乐" target="_blank" style="color:#C20C0C"><span class="icon iconify" data-icon="ri:netease-cloud-music-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://space.bilibili.com/22660607" title="哔哩哔哩" target="_blank" style="color:#FF8EB3"><span class="icon iconify" data-icon="ri:bilibili-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="mailto:charliedu2000@hotmail.com" title="E-Mail" target="_blank" style="color:#8E71C1"><span class="icon iconify" data-icon="ri:mail-line"></span></a></div><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="旁友们" style="color:dodgerblue"><span class="icon iconify" data-icon="ri:genderless-line"></span></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><span class="icon iconify" data-icon="ri:contrast-2-line"></span></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B3%E4%BA%8E%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="toc-number">1.</span> <span class="toc-text">关于计算图</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E5%9B%BE%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">1.1.</span> <span class="toc-text">计算图与反向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B1%80%E9%83%A8%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%85%B6%E4%BB%96"><span class="toc-number">1.2.</span> <span class="toc-text">局部计算与其他</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99"><span class="toc-number">2.</span> <span class="toc-text">链式法则</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E5%9B%BE%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">2.1.</span> <span class="toc-text">计算图的反向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99%E7%9A%84%E5%8E%9F%E7%90%86"><span class="toc-number">2.2.</span> <span class="toc-text">链式法则的原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99%E5%92%8C%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="toc-number">2.3.</span> <span class="toc-text">链式法则和计算图</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">3.</span> <span class="toc-text">反向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E6%B3%95%E8%8A%82%E7%82%B9%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">3.1.</span> <span class="toc-text">加法节点的反向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B9%98%E6%B3%95%E8%8A%82%E7%82%B9%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">3.2.</span> <span class="toc-text">乘法节点的反向传播</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%80%E5%8D%95%E5%B1%82%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">4.</span> <span class="toc-text">简单层的实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B9%98%E6%B3%95%E5%B1%82%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">4.1.</span> <span class="toc-text">乘法层的实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E6%B3%95%E5%B1%82%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">4.2.</span> <span class="toc-text">加法层的实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%B1%82%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">5.</span> <span class="toc-text">激活函数层的实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ReLU-%E5%B1%82"><span class="toc-number">5.1.</span> <span class="toc-text">ReLU 层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Sigmoid-%E5%B1%82"><span class="toc-number">5.2.</span> <span class="toc-text">Sigmoid 层</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Affine-x2F-Softmax-%E5%B1%82"><span class="toc-number">6.</span> <span class="toc-text">Affine&#x2F;Softmax 层</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Affine-%E5%B1%82"><span class="toc-number">6.1.</span> <span class="toc-text">Affine 层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Softmax-with-loss-%E5%B1%82"><span class="toc-number">6.2.</span> <span class="toc-text">Softmax-with-loss 层</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%AF%E5%B7%AE%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%B3%95%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">7.</span> <span class="toc-text">误差反向传播法的实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%AF%E5%B7%AE%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%B3%95%E7%9A%84%E6%A2%AF%E5%BA%A6%E7%A1%AE%E8%AE%A4"><span class="toc-number">7.1.</span> <span class="toc-text">误差反向传播法的梯度确认</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E8%AF%AF%E5%B7%AE%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%B3%95%E7%9A%84%E5%AD%A6%E4%B9%A0"><span class="toc-number">7.2.</span> <span class="toc-text">使用误差反向传播法的学习</span></a></li></ol></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article" style="--smc-primary:#0078E7;"><link itemprop="mainEntityOfPage" href="https://afool.top/learning/deep-learning-from-scratch-error-back-propagation/"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="零歌"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="愚人而已"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">深度学习入门：误差反向传播法</h1><div class="post-meta"><div class="post-time" style="display:block"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-line"></span></span> <time title="创建时间：2022-10-17 23:06:12" itemprop="dateCreated datePublished" datetime="2022-10-17T23:06:12+08:00">2022-10-17</time></div><span class="post-count"><span class="post-symbolcount"><span class="post-meta-item-icon" title="本文字数"><span class="icon iconify" data-icon="ri:file-word-line"></span></span> <span title="本文字数">4.2k</span><span class="post-meta-divider">-</span><span class="post-meta-item-icon" title="阅读时长"><span class="icon iconify" data-icon="ri:timer-line"></span></span> <span title="阅读时长">18m</span></span></span><div class="post-classify"><span class="post-category"> <span class="post-meta-item-icon" style="margin-right:3px;"><span class="icon iconify" data-icon="ri:folder-line"></span></span><span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category-item" href="/categories/%E5%BF%AB%E5%8E%BB%E5%AD%A6%E4%B9%A0/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">快去学习</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag-item" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="tag-name">深度学习</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body"><p>内容参考：</p>
<ul>
<li>《深度学习入门：基于 Python 的理论与实现》(斋藤康毅)</li>
<li><a target="_blank" rel="noopener" href="https://github.com/oreilly-japan/deep-learning-from-scratch">上述书籍作者提供的代码</a></li>
</ul>
<p>之前成功实现了一个神经网络的学习过程，但是实际开始计算就会发现整个过程实在是太慢了……对所有参数都要计算梯度，而且要计算很多遍，显然我们需要一种能更快地计算出权重参数梯度的方法。</p>
<h2 id="关于计算图"><a href="#关于计算图" class="headerlink" title="关于计算图"></a>关于计算图</h2><p>计算图能够将计算过程用图形（数据结构图，用多个节点和边表示）表示出来。</p>
<h3 id="计算图与反向传播"><a href="#计算图与反向传播" class="headerlink" title="计算图与反向传播"></a>计算图与反向传播</h3><p>使用计算图是为了直观理解反向传播法，那至少应该能先看懂计算图才行。</p>
<p>书上的例题1：太郎在超市买了2个100日元一个的苹果，消费税是10%，请计算支付金额。过程很简单，先算出2个苹果的价格，再算出加上消费税之后的花销。用图表示：</p>
<p><img src="https://s2.loli.net/2022/10/18/4hJ7QlSEdDjKzn3.png" alt="基于计算图的答案" loading="lazy"></p>
<p>在这张图中，苹果单价流到“$\times 2$”节点，又被传递给“$\times 1.1$”节点，得出最后的价格。图中“$\times 2$”和“$\times 1.1$”是作为一个运算的整体，也可以只用 $\circ$ 表示乘法，将“2”和“1.1”等作为变量标在外面：</p>
<p><img src="https://s2.loli.net/2022/10/18/iW1Ge4oqmv9Pb27.png" alt="基于计算图的答案-将“苹果的个数”和“消费税”标在外面" loading="lazy"></p>
<p>依此类推，其他问题也可以用这个方法表示，比方说太郎再买若干橘子，可以把橘子个数和单价传给乘法的节点，再把算出的橘子价格和苹果价格都传到一个加法节点，将总的价格拿去算消费税。用计算图解题的步骤可以归纳为：</p>
<ol>
<li>构建计算图；</li>
<li>在计算图上从左到右进行计算。</li>
</ol>
<p>上面“从左到右进行计算”的过程就是所谓<strong>正向传播</strong>，那么从图上从右向左就是<strong>反向传播</strong>。</p>
<h3 id="局部计算与其他"><a href="#局部计算与其他" class="headerlink" title="局部计算与其他"></a>局部计算与其他</h3><p>尽管问题的全局可能非常复杂，但是计算图能够分节点将整个计算过程分解成简单的计算。</p>
<p>此外，可以借助计算图反向传播高效计算导数。比方说前面计算了购买2个苹果时加上消费税最终需要支付的金额，如果想知道道苹果价格的上涨会在多大程度上影响最终的支付金额（即支付金额关于苹果价格的导数），可以看这张图：</p>
<p><img src="https://s2.loli.net/2022/10/20/RlpfhqoJxEeiCaH.png" alt="基于反向传播的导数的传递" loading="lazy"></p>
<p>反向传播传递“局部导数”，在这个例子中反向传播从右向左传递导数的值（1、1.1、2.2），由此可知支付金额关于苹果价格的导数值是2.2。</p>
<p>使用这种方法，中间求得的导数结果可以被共享，从而提升计算效率。</p>
<h2 id="链式法则"><a href="#链式法则" class="headerlink" title="链式法则"></a>链式法则</h2><p>反向传播传递局部导数的原理是基于<strong>链式法则</strong>的，它如何对应计算图上的反向传播呢？</p>
<h3 id="计算图的反向传播"><a href="#计算图的反向传播" class="headerlink" title="计算图的反向传播"></a>计算图的反向传播</h3><p>假设有 $y&#x3D;f(x)$，它的反向传播：</p>
<p><img src="https://s2.loli.net/2022/10/20/QnoPifB5gbxHaW1.png" alt="计算图的反向传播-沿着反方向乘上局部导数" loading="lazy"></p>
<p>如图所示，反向传播的计算顺序是将上游传过来的值（信号 $E$）乘以节点的局部导数（这里是 $y$ 关于 $x$ 的导数 $\frac{\partial y}{\partial x}$）再传给下一个节点。</p>
<h3 id="链式法则的原理"><a href="#链式法则的原理" class="headerlink" title="链式法则的原理"></a>链式法则的原理</h3><p>链式法则要先从复合函数说起，复合函数就是由多个函数构成的函数。例如 $z &#x3D; (x+y)^2$ 是由下面两个式子构成的：</p>
<p>$$ z &#x3D; t^2 $$</p>
<p>$$ t &#x3D; x + y $$</p>
<p>链式法则的定义：<em>如果某个函数由复合函数表示，则该复合函数的导数可以用构成复合函数的各个函数的导数的乘积表示</em>。也就是说，对于上面的复合函数，有：</p>
<p>$$ \frac{\partial z}{\partial x} &#x3D; \frac{\partial z}{\partial t} \frac{\partial t}{\partial x} $$</p>
<p>尝试使用链式法则求 $\frac{\partial z}{\partial x}$，有：</p>
<p>$$ \frac{\partial z}{\partial t} &#x3D; 2 t $$</p>
<p>$$ \frac{\partial t}{\partial x} &#x3D; 1 $$</p>
<p>那么：</p>
<p>$$ \frac{\partial z}{\partial x} &#x3D; \frac{\partial z}{\partial t} \frac{\partial t}{\partial x} &#x3D; 2t \cdot 1 &#x3D; 2 (x + y) $$</p>
<h3 id="链式法则和计算图"><a href="#链式法则和计算图" class="headerlink" title="链式法则和计算图"></a>链式法则和计算图</h3><p>用计算图表示上面的计算过程：</p>
<p><img src="https://s2.loli.net/2022/10/20/a6QUWR3ypTjJqDm.png" alt="计算图表示链式法则的计算过程，**2表示平方" loading="lazy"></p>
<p>以“**2”节点为例，正向传播时输入为 $t$，输出为 $z$，所以局部导数为 $\frac{\partial z}{\partial t}$，反向传播时的输入为 $\frac{\partial z}{\partial z}$（1），将其乘以局部导数再传给下一个节点。最终代入各个局部导数可以得到结果。</p>
<h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><h3 id="加法节点的反向传播"><a href="#加法节点的反向传播" class="headerlink" title="加法节点的反向传播"></a>加法节点的反向传播</h3><p>以 $z &#x3D; x + y$ 为例，它的导数：</p>
<p>$$ \frac{\partial z}{\partial x} &#x3D; 1 $$</p>
<p>$$ \frac{\partial z}{\partial y} &#x3D; 1 $$</p>
<p>用计算图表示（$\frac{\partial L}{\partial z}$ 表示反向传播时上游传过来的导数）：</p>
<p><img src="https://s2.loli.net/2022/10/20/EM6uPf52a3kHmCc.png" alt="加法节点的反向传播" loading="lazy"></p>
<p>在本例中，因为 $\frac{\partial z}{\partial x}$ 和 $\frac{\partial z}{\partial y}$ 都是1，加法节点就是把上游传过来的导数乘以1再传给下一个节点。</p>
<h3 id="乘法节点的反向传播"><a href="#乘法节点的反向传播" class="headerlink" title="乘法节点的反向传播"></a>乘法节点的反向传播</h3><p>以 $z &#x3D; x y$ 为例，它的导数：</p>
<p>$$ \frac{\partial z}{\partial x} &#x3D; y $$</p>
<p>$$ \frac{\partial z}{\partial y} &#x3D; x $$</p>
<p>画出计算图：</p>
<p><img src="https://s2.loli.net/2022/10/20/5Qo7dfV9tECyL1M.png" alt="乘法节点的反向传播" loading="lazy"></p>
<p>如果正向的时候信号是 $x$，反向的时候局部导数就是 $y$；正向的时候信号是 $y$，反向的时候局部导数就是 $x$。表现为一种“翻转”的关系。乘法节点需要保留正向传播时传入的参数信息。</p>
<h2 id="简单层的实现"><a href="#简单层的实现" class="headerlink" title="简单层的实现"></a>简单层的实现</h2><p>知道了反向传播是怎么回事，不妨再来看看怎么用代码实现相关的乘法和加法节点（“层”）。</p>
<h3 id="乘法层的实现"><a href="#乘法层的实现" class="headerlink" title="乘法层的实现"></a>乘法层的实现</h3><p>对于 $z &#x3D; x y$，有实现：</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MulLayer</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>x <span class="token operator">=</span> <span class="token boolean">None</span>
        self<span class="token punctuation">.</span>y <span class="token operator">=</span> <span class="token boolean">None</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>x <span class="token operator">=</span> x
        self<span class="token punctuation">.</span>y <span class="token operator">=</span> y
        out <span class="token operator">=</span> x <span class="token operator">*</span> y
        <span class="token keyword">return</span> out

    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dout<span class="token punctuation">)</span><span class="token punctuation">:</span>
        dx <span class="token operator">=</span> dout <span class="token operator">*</span> self<span class="token punctuation">.</span>y  <span class="token comment"># 翻转 x 和 y</span>
        dy <span class="token operator">=</span> dout <span class="token operator">*</span> self<span class="token punctuation">.</span>x
        <span class="token keyword">return</span> dx<span class="token punctuation">,</span> dy</code></pre>

<p>用这个乘法层实现最开始说的买苹果的问题，计算图如下：</p>
<p><img src="https://s2.loli.net/2022/10/20/IJtF3jzyrCfnOQ9.png" alt="买两个苹果" loading="lazy"></p>
<p>正向传播的计算过程：</p>
<pre class="language-python" data-language="python"><code class="language-python">apple <span class="token operator">=</span> <span class="token number">100</span>
apple_num <span class="token operator">=</span> <span class="token number">2</span>
tax <span class="token operator">=</span> <span class="token number">1.1</span>

mul_apple_layer <span class="token operator">=</span> MulLayer<span class="token punctuation">(</span><span class="token punctuation">)</span>
mul_tax_layer <span class="token operator">=</span> MulLayer<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 正向传播过程</span>
apple_price <span class="token operator">=</span> mul_apple_layer<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>apple<span class="token punctuation">,</span> apple_num<span class="token punctuation">)</span>
price <span class="token operator">=</span> mul_tax_layer<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>apple_price<span class="token punctuation">,</span> tax<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>price<span class="token punctuation">)</span>  <span class="token comment"># 结果是220.00000000000003</span></code></pre>

<p>反向传播求过程中各个变量的导数：</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># 反向传播过程</span>
dprice <span class="token operator">=</span> <span class="token number">1</span>
dapple_price<span class="token punctuation">,</span> dtax <span class="token operator">=</span> mul_tax_layer<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>dprice<span class="token punctuation">)</span>
dapple<span class="token punctuation">,</span> dapple_num <span class="token operator">=</span> mul_apple_layer<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>dapple_price<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>dapple<span class="token punctuation">,</span> dapple_num<span class="token punctuation">,</span> dtax<span class="token punctuation">)</span>  <span class="token comment"># 结果是 2.2 110.00000000000001 200</span></code></pre>

<h3 id="加法层的实现"><a href="#加法层的实现" class="headerlink" title="加法层的实现"></a>加法层的实现</h3><p>对于 $z &#x3D; x + y$，加法层有实现如下：</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">AddLayer</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        <span class="token keyword">pass</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>
        out <span class="token operator">=</span> x <span class="token operator">+</span> y
        <span class="token keyword">return</span> out
 
    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dout<span class="token punctuation">)</span><span class="token punctuation">:</span>
        dx <span class="token operator">=</span> dout <span class="token operator">*</span> <span class="token number">1</span>
        dy <span class="token operator">=</span> dout <span class="token operator">*</span> <span class="token number">1</span>
        <span class="token keyword">return</span> dx<span class="token punctuation">,</span> dy</code></pre>

<p>要解决的问题是买2个苹果和3个橘子，计算图如下：</p>
<p><img src="https://s2.loli.net/2022/10/21/KieElhcZmUMOJCQ.png" alt="买2个苹果和3个橘子" loading="lazy"></p>
<p>正向传播和反向传播的过程：</p>
<pre class="language-python" data-language="python"><code class="language-python">apple <span class="token operator">=</span> <span class="token number">100</span>
apple_num <span class="token operator">=</span> <span class="token number">2</span>
orange <span class="token operator">=</span> <span class="token number">150</span>
orange_num <span class="token operator">=</span> <span class="token number">3</span>
tax <span class="token operator">=</span> <span class="token number">1.1</span>

mul_apple_layer <span class="token operator">=</span> MulLayer<span class="token punctuation">(</span><span class="token punctuation">)</span>
mul_orange_layer <span class="token operator">=</span> MulLayer<span class="token punctuation">(</span><span class="token punctuation">)</span>
add_apple_orange_layer <span class="token operator">=</span> AddLayer<span class="token punctuation">(</span><span class="token punctuation">)</span>
mul_tax_layer <span class="token operator">=</span> MulLayer<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 前向传播过程</span>
apple_price <span class="token operator">=</span> mul_apple_layer<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>apple<span class="token punctuation">,</span> apple_num<span class="token punctuation">)</span>  <span class="token comment">#(1)</span>
orange_price <span class="token operator">=</span> mul_orange_layer<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>orange<span class="token punctuation">,</span> orange_num<span class="token punctuation">)</span>  <span class="token comment">#(2)</span>
all_price <span class="token operator">=</span> add_apple_orange_layer<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>apple_price<span class="token punctuation">,</span> orange_price<span class="token punctuation">)</span>  <span class="token comment">#(3)</span>
price <span class="token operator">=</span> mul_tax_layer<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>all_price<span class="token punctuation">,</span> tax<span class="token punctuation">)</span>  <span class="token comment">#(4)</span>

<span class="token comment"># 反向传播过程，要注意与正向传播的计算顺序相反</span>
dprice <span class="token operator">=</span> <span class="token number">1</span>
dall_price<span class="token punctuation">,</span> dtax <span class="token operator">=</span> mul_tax_layer<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>dprice<span class="token punctuation">)</span>  <span class="token comment">#(4)</span>
dapple_price<span class="token punctuation">,</span> dorange_price <span class="token operator">=</span> add_apple_orange_layer<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>dall_price<span class="token punctuation">)</span>  <span class="token comment">#(3)</span>
dorange<span class="token punctuation">,</span> dorange_num <span class="token operator">=</span> mul_orange_layer<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>dorange_price<span class="token punctuation">)</span>  <span class="token comment">#(2)</span>
dapple<span class="token punctuation">,</span> dapple_num <span class="token operator">=</span> mul_apple_layer<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>dapple_price<span class="token punctuation">)</span>  <span class="token comment">#(1)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>price<span class="token punctuation">)</span>  <span class="token comment"># 结果是715.0000000000001</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>dapple_num<span class="token punctuation">,</span> dapple<span class="token punctuation">,</span> dorange<span class="token punctuation">,</span> dorange_num<span class="token punctuation">,</span>
      dtax<span class="token punctuation">)</span>  <span class="token comment"># 结果是110.00000000000001 2.2 3.3000000000000003 165.0 650</span></code></pre>

<p>与计算图对照，显然能得到正确结果。</p>
<h2 id="激活函数层的实现"><a href="#激活函数层的实现" class="headerlink" title="激活函数层的实现"></a>激活函数层的实现</h2><p>既然简单的乘法加法等节点能用代码以层的形式实现，那么神经网络中的节点应该也可以，毕竟也没有复杂太多。把构成神经网络的层实现为一个类。先来实现激活函数的 ReLU 层和 Sigmoid 层。</p>
<h3 id="ReLU-层"><a href="#ReLU-层" class="headerlink" title="ReLU 层"></a>ReLU 层</h3><p>ReLU 函数：</p>
<p>$$ y &#x3D; \begin{cases} x &amp; (x \gt 0) \\ 0 &amp; (x \le 0) \end{cases} $$</p>
<p>$y$ 关于 $x$ 的导数：</p>
<p>$$ \frac{\partial y}{\partial x} &#x3D; \begin{cases} 1 &amp; (x \gt 0) \\ 0 &amp; (x \le 0) \end{cases} $$</p>
<p>那么，ReLU 层的计算图：</p>
<p><img src="https://s2.loli.net/2022/10/21/LCrvOc4AmQhdpgz.png" alt="ReLU 层的计算图" loading="lazy"></p>
<p>代码实现：</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Relu</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>mask <span class="token operator">=</span> <span class="token boolean">None</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>mask <span class="token operator">=</span> <span class="token punctuation">(</span>x <span class="token operator">&lt;=</span> <span class="token number">0</span><span class="token punctuation">)</span>  <span class="token comment"># mask 是由 bool 值组成的 numpy 数组</span>
        out <span class="token operator">=</span> x<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>
        out<span class="token punctuation">[</span>self<span class="token punctuation">.</span>mask<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>  <span class="token comment"># 小于等于0的项置0</span>
        <span class="token keyword">return</span> out

    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dout<span class="token punctuation">)</span><span class="token punctuation">:</span>
        dout<span class="token punctuation">[</span>self<span class="token punctuation">.</span>mask<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>  <span class="token comment"># 正向传播时输入小于等于0的项对应导数是0</span>
        dx <span class="token operator">=</span> dout
        <span class="token keyword">return</span> dx</code></pre>

<h3 id="Sigmoid-层"><a href="#Sigmoid-层" class="headerlink" title="Sigmoid 层"></a>Sigmoid 层</h3><p>Sigmoid 函数：</p>
<p>$$ y &#x3D; \frac{1}{1 + exp(-x)} $$</p>
<p>用计算图表示正向传播过程：</p>
<p><img src="https://s2.loli.net/2022/10/21/TZjQGdIFMoLEUvO.png" alt="Sigmoid 函数的正向传播" loading="lazy"></p>
<p>Sigmoid 函数的计算过程中出现了新的节点，需要看一看它们如何进行反向传播。</p>
<p>最后的“&#x2F;”节点表示 $y &#x3D; \frac{1}{x}$，它的导数可以解析性地表示为：</p>
<p>$$ \begin{aligned} \frac{\partial y}{\partial x} &amp;&#x3D; - \frac{1}{x^2} \\ &amp;&#x3D; - y^2 \end{aligned} $$</p>
<p>也就是说，反向传播时会将上游传过来的值乘以 $-y^2$（$y$ 是正向传播时该节点的输出）再传给下游。</p>
<p>这个加法节点反向传播时显然是将上游传过来的值直接传过去。</p>
<p>“exp” 节点表示 $y &#x3D; exp(x)$，导数如下：</p>
<p>$$ \frac{\partial y}{\partial x} &#x3D; exp(x) $$</p>
<p>正向传播时传入的是 $-x$，反向传播时就把上游传过来的值乘以 $exp(-x)$ 再传给下游。</p>
<p>乘法节点是 $x$ 与 $1$ 相乘，所以反向传播时传给 $x$ 方向的应该是上游的值乘以 $-1$。</p>
<p>包含反向传播的计算图如下：</p>
<p><img src="https://s2.loli.net/2022/10/21/Zwpf3F4E8xglh1u.png" alt="Sigmoid 函数的计算图" loading="lazy"></p>
<p>由图可知反向传播的输出是 $\frac{\partial L}{\partial y} y^2 exp(-x)$，只根据正向传播的输入 $x$ 和输出 $y$ 就能得出，所以计算图可以简化：</p>
<p><img src="https://s2.loli.net/2022/10/21/f2YUTkF5xceru6M.png" alt="简化后 Sigmoid 函数的计算图" loading="lazy"></p>
<p>又因为 $y &#x3D; \frac{1}{1 + exp(-x)}$，$\frac{\partial L}{\partial y} y^2 exp(-x)$ 可以被简化为：</p>
<p>$$ \begin{aligned} \frac{\partial L}{\partial y} y^2 exp(-x) &amp;&#x3D; \frac{\partial L}{\partial y} \frac{1}{(1 + exp(-x))^2} exp(-x) \\ &amp;&#x3D; \frac{\partial L}{\partial y} \frac{1}{1 + exp(-x)} \frac{exp(-x)}{1 + exp(-x)} \\ &amp;&#x3D; \frac{\partial L}{\partial y} y(1 - y) \end{aligned} $$</p>
<p>也就是说，Sigmoid 层的反向传播的输出，根据它正向传播时的输出就能算出来。</p>
<p>Sigmoid 层的代码实现：</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Sigmoid</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>out <span class="token operator">=</span> <span class="token boolean">None</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        out <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>out <span class="token operator">=</span> out
        <span class="token keyword">return</span> out

    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dout<span class="token punctuation">)</span><span class="token punctuation">:</span>
        dx <span class="token operator">=</span> dout <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>out<span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>out  <span class="token comment"># 根据正向传播的输出计算反向传播的结果</span>
        <span class="token keyword">return</span> dx</code></pre>

<h2 id="Affine-x2F-Softmax-层"><a href="#Affine-x2F-Softmax-层" class="headerlink" title="Affine&#x2F;Softmax 层"></a>Affine&#x2F;Softmax 层</h2><p><del>Affine 层进行仿射变换，也就是矩阵乘积？（实际上还要算上与偏置之和？）</del></p>
<p>用层的方式实现矩阵乘积和 Softmax 节点。</p>
<h3 id="Affine-层"><a href="#Affine-层" class="headerlink" title="Affine 层"></a>Affine 层</h3><p>矩阵相乘时对应维度的元素个数必须一致，比如：</p>
<p><img src="https://s2.loli.net/2022/10/23/kD8buZ2w9QaBOor.png" alt="矩阵相乘时对应维度的元素个数应该一致" loading="lazy"></p>
<p>求加权信号的和包括计算矩阵乘积以及与偏置求和。用计算图表示这些运算，其中用“dot”节点表示矩阵乘积：</p>
<p><img src="https://s2.loli.net/2022/10/23/ngJBVy6RGq2o4x3.png" alt="计算图-求加权信号之和" loading="lazy"></p>
<p>各个节点间传播的是矩阵。</p>
<p>考虑上图“dot”节点的反向传播，有：</p>
<p>$$ \frac{\partial L}{\partial \boldsymbol{X}} &#x3D; \frac{\partial L}{\partial \boldsymbol{Y}} \cdot \boldsymbol{W}^T $$</p>
<p>$$ \frac{\partial L}{\partial \boldsymbol{W}} &#x3D; \boldsymbol{X}^T \cdot \frac{\partial L}{\partial \boldsymbol{Y}} $$</p>
<p>那么上面计算图的反向传播：</p>
<p><img src="https://s2.loli.net/2022/10/28/wgac9kBEe8UbGQs.png" alt="计算图的反向传播" loading="lazy"></p>
<p>如果输入数据不是单个数据而是批，反向传播时矩阵形状会有所不同：</p>
<p><img src="https://s2.loli.net/2022/10/28/F3Yjm1lEyh2nNVP.png" alt="计算图的反向传播-batch" loading="lazy"></p>
<p>一种代码实现：</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Affine</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> W<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>W <span class="token operator">=</span> W
        self<span class="token punctuation">.</span>b <span class="token operator">=</span> b
        self<span class="token punctuation">.</span>x <span class="token operator">=</span> <span class="token boolean">None</span>
        self<span class="token punctuation">.</span>dW <span class="token operator">=</span> <span class="token boolean">None</span>
        self<span class="token punctuation">.</span>db <span class="token operator">=</span> <span class="token boolean">None</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>x <span class="token operator">=</span> x
        out <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>W<span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>b
        <span class="token keyword">return</span> out

    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dout<span class="token punctuation">)</span><span class="token punctuation">:</span>
        dx <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>dout<span class="token punctuation">,</span> self<span class="token punctuation">.</span>W<span class="token punctuation">.</span>T<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dW <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>self<span class="token punctuation">.</span>x<span class="token punctuation">.</span>T<span class="token punctuation">,</span> dout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>db <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>dout<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> dx</code></pre>

<h3 id="Softmax-with-loss-层"><a href="#Softmax-with-loss-层" class="headerlink" title="Softmax-with-loss 层"></a>Softmax-with-loss 层</h3><p>最后是输出层的 softmax 函数，考虑作为损失函数的交叉熵误差。</p>
<p>假设要进行3类分类，从前面的层接收3个输入。Softmax 层将输入 $(a_1, a_2, a_3)$ 正规化，输出 $(y_1, y_2, y_3)$。Cross Entropy Error 层接收 Softmax 的输出和监督数据 $(t_1,<br>t_2, t_3)$，输出损失 $L$。简化后的计算图：</p>
<p><img src="https://s2.loli.net/2022/10/30/cjdiZV8CTMteoYR.png" alt="简化后的 Softmax-with-loss 层的计算图" loading="lazy"></p>
<p>实现过程：</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">SoftmaxWithLoss</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>loss <span class="token operator">=</span> <span class="token boolean">None</span>  <span class="token comment"># 损失</span>
        self<span class="token punctuation">.</span>y <span class="token operator">=</span> <span class="token boolean">None</span>  <span class="token comment"># softmax 的输出</span>
        self<span class="token punctuation">.</span>t <span class="token operator">=</span> <span class="token boolean">None</span>  <span class="token comment"># 监督数据（one-hot vector）</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>t <span class="token operator">=</span> t
        self<span class="token punctuation">.</span>y <span class="token operator">=</span> softmax<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>loss <span class="token operator">=</span> cross_entropy_error<span class="token punctuation">(</span>self<span class="token punctuation">.</span>y<span class="token punctuation">,</span> self<span class="token punctuation">.</span>t<span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>loss

    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dout<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        batch_size <span class="token operator">=</span> self<span class="token punctuation">.</span>t<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        dx <span class="token operator">=</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>y <span class="token operator">-</span> self<span class="token punctuation">.</span>t<span class="token punctuation">)</span> <span class="token operator">/</span> batch_size  <span class="token comment"># 单个数据的误差</span>
        <span class="token keyword">return</span> dx</code></pre>

<h2 id="误差反向传播法的实现"><a href="#误差反向传播法的实现" class="headerlink" title="误差反向传播法的实现"></a>误差反向传播法的实现</h2><p>已经知道神经网络学习的步骤如下：</p>
<blockquote>
<ul>
<li><strong>前提</strong><ul>
<li>神经网络存在合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为“学习”。神经网络的学习分成下面4个步骤。</li>
</ul>
</li>
<li><strong>步骤1（mini-batch）</strong><ul>
<li>从训练数据中随机选出一部分数据。</li>
</ul>
</li>
<li><strong>步骤2（计算梯度）</strong><ul>
<li>求出各个权重参数的梯度。</li>
</ul>
</li>
<li><strong>步骤3（更新参数）</strong><ul>
<li>将权重参数沿梯度方向进行微小更新。</li>
</ul>
</li>
<li><strong>步骤4（重复）</strong><ul>
<li>重复步骤1、步骤2、步骤3</li>
</ul>
</li>
</ul>
</blockquote>
<p>误差反向传播法在第二步中出现。之前计算梯度是通过数值微分求的，实现简单，不过计算起来要花费比较长的时间，用误差反向传播法可以改善这一点。</p>
<p>我们已经将网络中的节点表示成了“层”，网络获取结果和计算梯度的过程可以由层之间的传递完成。对应误差反向传播法的2层网络的实现：</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> sys<span class="token punctuation">,</span> os
sys<span class="token punctuation">.</span>path<span class="token punctuation">.</span>append<span class="token punctuation">(</span>os<span class="token punctuation">.</span>curdir<span class="token punctuation">)</span>
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">from</span> common<span class="token punctuation">.</span>layers <span class="token keyword">import</span> <span class="token operator">*</span>
<span class="token keyword">from</span> common<span class="token punctuation">.</span>gradient <span class="token keyword">import</span> numerical_gradient
<span class="token keyword">from</span> collections <span class="token keyword">import</span> OrderedDict  <span class="token comment"># 有序字典</span>


<span class="token keyword">class</span> <span class="token class-name">TwoLayerNet</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>
                 input_size<span class="token punctuation">,</span>
                 hidden_size<span class="token punctuation">,</span>
                 output_size<span class="token punctuation">,</span>
                 weight_init_std<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        <span class="token comment"># 初始化权重</span>
        self<span class="token punctuation">.</span>params <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>
        self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'W1'</span><span class="token punctuation">]</span> <span class="token operator">=</span> weight_init_std <span class="token operator">*</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>
            input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'b1'</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>hidden_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'W2'</span><span class="token punctuation">]</span> <span class="token operator">=</span> weight_init_std <span class="token operator">*</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>
            hidden_size<span class="token punctuation">,</span> output_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'b2'</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>output_size<span class="token punctuation">)</span>

        <span class="token comment"># 生成层</span>
        self<span class="token punctuation">.</span>layers <span class="token operator">=</span> OrderedDict<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>layers<span class="token punctuation">[</span><span class="token string">'Affine1'</span><span class="token punctuation">]</span> <span class="token operator">=</span> Affine<span class="token punctuation">(</span>self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'W1'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'b1'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>layers<span class="token punctuation">[</span><span class="token string">'ReLU'</span><span class="token punctuation">]</span> <span class="token operator">=</span> Relu<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>layers<span class="token punctuation">[</span><span class="token string">'Affine2'</span><span class="token punctuation">]</span> <span class="token operator">=</span> Affine<span class="token punctuation">(</span>self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'W2'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'b2'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>lastLayer <span class="token operator">=</span> SoftmaxWithLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">predict</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            x <span class="token operator">=</span> layer<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        <span class="token keyword">return</span> x

    <span class="token keyword">def</span> <span class="token function">loss</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">:</span>
        y <span class="token operator">=</span> self<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>lastLayer<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>y<span class="token punctuation">,</span> t<span class="token punctuation">)</span>  <span class="token comment"># 算出交叉熵误差</span>

    <span class="token keyword">def</span> <span class="token function">accuracy</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">:</span>
        y <span class="token operator">=</span> self<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        y <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>y<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> t<span class="token punctuation">.</span>ndim <span class="token operator">!=</span> <span class="token number">1</span><span class="token punctuation">:</span>
            t <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>t<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        accuracy <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>y <span class="token operator">==</span> t<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token builtin">float</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> accuracy

    <span class="token keyword">def</span> <span class="token function">numerical_gradient</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">:</span>
        loss_W <span class="token operator">=</span> <span class="token keyword">lambda</span> W<span class="token punctuation">:</span> self<span class="token punctuation">.</span>loss<span class="token punctuation">(</span>x<span class="token punctuation">,</span> t<span class="token punctuation">)</span>

        <span class="token comment"># 对每层的参数求梯度</span>
        grads <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>
        grads<span class="token punctuation">[</span><span class="token string">'W1'</span><span class="token punctuation">]</span> <span class="token operator">=</span> numerical_gradient<span class="token punctuation">(</span>loss_W<span class="token punctuation">,</span> self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'W1'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        grads<span class="token punctuation">[</span><span class="token string">'b1'</span><span class="token punctuation">]</span> <span class="token operator">=</span> numerical_gradient<span class="token punctuation">(</span>loss_W<span class="token punctuation">,</span> self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'b1'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        grads<span class="token punctuation">[</span><span class="token string">'W2'</span><span class="token punctuation">]</span> <span class="token operator">=</span> numerical_gradient<span class="token punctuation">(</span>loss_W<span class="token punctuation">,</span> self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'W2'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        grads<span class="token punctuation">[</span><span class="token string">'b2'</span><span class="token punctuation">]</span> <span class="token operator">=</span> numerical_gradient<span class="token punctuation">(</span>loss_W<span class="token punctuation">,</span> self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'b2'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> grads

    <span class="token keyword">def</span> <span class="token function">gradient</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 前向传播</span>
        self<span class="token punctuation">.</span>loss<span class="token punctuation">(</span>x<span class="token punctuation">,</span> t<span class="token punctuation">)</span>

        <span class="token comment"># 反向</span>
        dout <span class="token operator">=</span> <span class="token number">1</span>
        dout <span class="token operator">=</span> self<span class="token punctuation">.</span>lastLayer<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>dout<span class="token punctuation">)</span>

        layers <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        layers<span class="token punctuation">.</span>reverse<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> layer <span class="token keyword">in</span> layers<span class="token punctuation">:</span>
            dout <span class="token operator">=</span> layer<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>dout<span class="token punctuation">)</span>

        grads <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>
        grads<span class="token punctuation">[</span><span class="token string">'W1'</span><span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">[</span><span class="token string">'Affine1'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>dW
        grads<span class="token punctuation">[</span><span class="token string">'b1'</span><span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">[</span><span class="token string">'Affine1'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>db
        grads<span class="token punctuation">[</span><span class="token string">'W2'</span><span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">[</span><span class="token string">'Affine2'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>dW
        grads<span class="token punctuation">[</span><span class="token string">'b2'</span><span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">[</span><span class="token string">'Affine2'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>db

        <span class="token keyword">return</span> grads</code></pre>

<h3 id="误差反向传播法的梯度确认"><a href="#误差反向传播法的梯度确认" class="headerlink" title="误差反向传播法的梯度确认"></a>误差反向传播法的梯度确认</h3><p>像上面这样通过反向传播，使用解析性的方法计算梯度，虽然免去了数值微分的大量计算，但是实现比较复杂（各层分别实现，都不能出错），还是需要经常比较数值微分的结果和误差反向传播法的结果，以确认误差反向传播法的结果是否正确。这个“确认”的操作就是<strong>梯度确认</strong>。</p>
<p>实现如下：</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> sys<span class="token punctuation">,</span> os

sys<span class="token punctuation">.</span>path<span class="token punctuation">.</span>append<span class="token punctuation">(</span>os<span class="token punctuation">.</span>curdir<span class="token punctuation">)</span>
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">from</span> dataset<span class="token punctuation">.</span>mnist <span class="token keyword">import</span> load_mnist
<span class="token keyword">from</span> two_layer_net <span class="token keyword">import</span> TwoLayerNet

<span class="token comment"># 读入数据</span>
<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> t_train<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> t_test<span class="token punctuation">)</span> <span class="token operator">=</span> load_mnist<span class="token punctuation">(</span>normalize<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                                  one_hot_label<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

network <span class="token operator">=</span> TwoLayerNet<span class="token punctuation">(</span>input_size<span class="token operator">=</span><span class="token number">784</span><span class="token punctuation">,</span> hidden_size<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span> output_size<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span>

x_batch <span class="token operator">=</span> x_train<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span>
t_batch <span class="token operator">=</span> t_train<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span>

grad_numerical <span class="token operator">=</span> network<span class="token punctuation">.</span>numerical_gradient<span class="token punctuation">(</span>x_batch<span class="token punctuation">,</span> t_batch<span class="token punctuation">)</span>
grad_backprop <span class="token operator">=</span> network<span class="token punctuation">.</span>gradient<span class="token punctuation">(</span>x_batch<span class="token punctuation">,</span> t_batch<span class="token punctuation">)</span>

<span class="token comment"># 每个权重梯度的绝对误差平均值</span>
<span class="token keyword">for</span> key <span class="token keyword">in</span> grad_numerical<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    diff <span class="token operator">=</span> np<span class="token punctuation">.</span>average<span class="token punctuation">(</span>np<span class="token punctuation">.</span><span class="token builtin">abs</span><span class="token punctuation">(</span>grad_backprop<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">-</span> grad_numerical<span class="token punctuation">[</span>key<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>key <span class="token operator">+</span> <span class="token string">':'</span> <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span>diff<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre>

<p>只要 <code>diff</code> 的值是一个接近0的很小的值就说明误差反向传播法的计算没有错误。</p>
<h3 id="使用误差反向传播法的学习"><a href="#使用误差反向传播法的学习" class="headerlink" title="使用误差反向传播法的学习"></a>使用误差反向传播法的学习</h3><p>与<a href="https://afool.top/learning/deep-learning-from-scratch-learning-of-neural-network/">上一篇</a>里的方法相比，不同之处就在于使用误差反向传播法来求梯度。实现如下：</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> sys<span class="token punctuation">,</span> os
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt

sys<span class="token punctuation">.</span>path<span class="token punctuation">.</span>append<span class="token punctuation">(</span>os<span class="token punctuation">.</span>curdir<span class="token punctuation">)</span>

<span class="token keyword">from</span> two_layer_net <span class="token keyword">import</span> TwoLayerNet
<span class="token keyword">from</span> dataset<span class="token punctuation">.</span>mnist <span class="token keyword">import</span> load_mnist

<span class="token comment"># print('Load mnist dataset...')</span>
<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> t_train<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> t_test<span class="token punctuation">)</span> <span class="token operator">=</span> load_mnist<span class="token punctuation">(</span>normalize<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                                  one_hot_label<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token comment"># print('Initialize network...')</span>
network <span class="token operator">=</span> TwoLayerNet<span class="token punctuation">(</span>input_size<span class="token operator">=</span><span class="token number">784</span><span class="token punctuation">,</span> hidden_size<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span> output_size<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span>

<span class="token comment"># 超参数</span>
iters_num <span class="token operator">=</span> <span class="token number">10000</span>  <span class="token comment"># 梯度法更新次数</span>
train_size <span class="token operator">=</span> x_train<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>  <span class="token comment"># 训练集大小</span>
batch_size <span class="token operator">=</span> <span class="token number">100</span>  <span class="token comment"># batch 大小</span>
learning_rate <span class="token operator">=</span> <span class="token number">0.1</span>  <span class="token comment"># 学习率</span>

train_loss_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
train_acc_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
test_acc_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

<span class="token comment"># 平均每个 epoch 的重复次数</span>
iter_per_epoch <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span>train_size <span class="token operator">/</span> batch_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>iters_num<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 获取 mini-batch</span>
    <span class="token comment"># print(i, ': choose mini-batch...')</span>
    batch_mask <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>choice<span class="token punctuation">(</span>train_size<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>
    x_batch <span class="token operator">=</span> x_train<span class="token punctuation">[</span>batch_mask<span class="token punctuation">]</span>
    t_batch <span class="token operator">=</span> t_train<span class="token punctuation">[</span>batch_mask<span class="token punctuation">]</span>

    <span class="token comment"># 计算梯度</span>
    <span class="token comment"># print(i, ': calculate grads...')</span>
    <span class="token comment"># grad = network.numerical_gradient(x_batch, t_batch)</span>
    grad <span class="token operator">=</span> network<span class="token punctuation">.</span>gradient<span class="token punctuation">(</span>x_batch<span class="token punctuation">,</span> t_batch<span class="token punctuation">)</span>  <span class="token comment"># 误差反向传播法</span>

    <span class="token comment"># 更新参数</span>
    <span class="token comment"># print(i, ': update params...')</span>
    <span class="token keyword">for</span> key <span class="token keyword">in</span> <span class="token punctuation">(</span><span class="token string">'W1'</span><span class="token punctuation">,</span> <span class="token string">'b1'</span><span class="token punctuation">,</span> <span class="token string">'W2'</span><span class="token punctuation">,</span> <span class="token string">'b2'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        network<span class="token punctuation">.</span>params<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">-=</span> learning_rate <span class="token operator">*</span> grad<span class="token punctuation">[</span>key<span class="token punctuation">]</span>

    <span class="token comment"># 记录学习过程</span>
    loss <span class="token operator">=</span> network<span class="token punctuation">.</span>loss<span class="token punctuation">(</span>x_batch<span class="token punctuation">,</span> t_batch<span class="token punctuation">)</span>
    <span class="token comment"># print(i, 'loss: ', loss)</span>
    train_loss_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>

    <span class="token comment"># 每个 epoch 完成后计算识别精度</span>
    <span class="token keyword">if</span> i <span class="token operator">%</span> iter_per_epoch <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
        train_acc <span class="token operator">=</span> network<span class="token punctuation">.</span>accuracy<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> t_train<span class="token punctuation">)</span>
        test_acc <span class="token operator">=</span> network<span class="token punctuation">.</span>accuracy<span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> t_test<span class="token punctuation">)</span>
        train_acc_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>train_acc<span class="token punctuation">)</span>
        test_acc_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>test_acc<span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Train acc, test acc | '</span> <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span>train_acc<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">', '</span> <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span>test_acc<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># 画出图像</span>
<span class="token comment"># markers = &#123;'train': 'o', 'test': 's'&#125;</span>
x <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>train_acc_list<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 生成 x 坐标</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> train_acc_list<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'train acc'</span><span class="token punctuation">)</span>  <span class="token comment"># 训练集识别精度</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> test_acc_list<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'test acc'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'--'</span><span class="token punctuation">)</span>  <span class="token comment"># 测试集识别精度，虚线</span>
plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">"epochs"</span><span class="token punctuation">)</span>  <span class="token comment"># 横轴单位</span>
plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">"accuracy"</span><span class="token punctuation">)</span>  <span class="token comment"># 纵轴单位</span>
plt<span class="token punctuation">.</span>ylim<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span>loc<span class="token operator">=</span><span class="token string">'lower right'</span><span class="token punctuation">)</span>  <span class="token comment"># 右下角图例</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre>

<p>效果立竿见影，比之前的快了不是一点半点。那就这样~</p>
</div></section><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>零歌</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="https://afool.top/learning/deep-learning-from-scratch-error-back-propagation/" title="深度学习入门：误差反向传播法">https://afool.top/learning/deep-learning-from-scratch-error-back-propagation/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><span class="icon iconify" data-icon="ri:creative-commons-line"></span><span class="icon iconify" data-icon="ri:creative-commons-by-line"></span><span class="icon iconify" data-icon="ri:creative-commons-nc-line"></span><span class="icon iconify" data-icon="ri:creative-commons-sa-line"></span></a> 许可协议。</li></ul></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/learning/deep-learning-from-scratch-some-tricks-related-with-learning/" rel="prev" title="深度学习入门：与学习相关的技巧"><span class="icon iconify" data-icon="ri:arrow-left-s-line"></span><span class="post-nav-text">深度学习入门：与学习相关的技巧</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/learning/deep-learning-from-scratch-learning-of-neural-network/" rel="next" title="深度学习入门：神经网络的学习"><span class="post-nav-text">深度学习入门：神经网络的学习</span><span class="icon iconify" data-icon="ri:arrow-right-s-line"></span></a></div></div></div><div class="hty-card" id="comment"><div class="comment-tooltip text-center"><span>留个爪印吧~</span><br></div><div id="tcomment"></div><script type="module">import { getScript } from '/js/utils.js'

getScript("https://fastly.jsdelivr.net/npm/twikoo@latest/dist/twikoo.all.min.js", () => {
  const twikooConfig = {"enable":true,"envId":"https://twikoo-afool.vercel.app/"}
  twikooConfig.el = '#tcomment'
  twikoo.init(twikooConfig)
}, window.twikoo);</script></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2020 – 2022 </span><span class="with-love" id="animate"><span class="icon iconify" data-icon="ri:cloud-line"></span></span><span class="author"> 零歌</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v6.3.0</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.10.9</span></div></footer></div><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><span class="icon iconify" data-icon="ri:arrow-up-s-line"></span><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a></body></html>