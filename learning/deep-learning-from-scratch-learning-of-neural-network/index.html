<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="零歌"><meta name="copyright" content="零歌"><meta name="generator" content="Hexo 6.3.0"><meta name="theme" content="hexo-theme-yun"><title>深度学习入门：神经网络的学习 | 愚人而已</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/star-markdown-css@0.4.1/dist/yun/yun-markdown.min.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/prism-theme-vars/base.css"><script src="https://fastly.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>function initScrollReveal() {
  [".post-card",".markdown-body img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
}
document.addEventListener("DOMContentLoaded", initScrollReveal);
document.addEventListener("pjax:success", initScrollReveal);
</script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"></script><script type="module">import { renderKatex } from '/js/utils.js'
document.addEventListener("DOMContentLoaded", () => {
  renderKatex({
    ...{},
    ...true?.options,
  });
});</script><link rel="icon" type="image/svg+xml" href="/afool.svg"><link rel="mask-icon" href="/afool.svg" color="#0078E7"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="preconnect" href="https://fastly.jsdelivr.net/npm/" crossorigin><script id="yun-config">
    window.Yun = {}
    window.CONFIG = {"hostname":"afool.top","root":"/","title":"愚人而已","version":"1.10.9","mode":"auto","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"搜索...","empty":"找不到您查询的内容: ${query}","hits":"找到 ${hits} 条结果","hits_time":"找到 ${hits} 条结果（用时 ${time} 毫秒）"},"anonymous_image":"https://cdn.yunyoujun.cn/img/avatar/none.jpg","say":{"api":"https://el-bot-api.vercel.app/api/words/young"},"fireworks":{"colors":null},"vendors":{"host":"https://fastly.jsdelivr.net/npm/","darken":"https://fastly.jsdelivr.net/npm/darken@1.5.0"}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/hexo-theme-yun.js" type="module"></script><link rel="alternate" href="/atom.xml" title="愚人而已" type="application/atom+xml"><meta name="description" content="内容参考：  《深度学习入门：基于 Python 的理论与实现》(斋藤康毅) 上述书籍作者提供的代码  之前实现了神经网络的前向传播过程，但是用到的权重参数是预先准备好的，那么接下来的目标自然就是“学习”——从训练数据中自动获取最优权重参数。 从数据中学习机器学习是以数据为核心的，尝试从数据中发现答案和模式。神经网络或深度学习则比以往的机器学习方法更能避免人为介入。 要理解上面这些描述还是要从具体">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习入门：神经网络的学习">
<meta property="og:url" content="https://afool.top/learning/deep-learning-from-scratch-learning-of-neural-network/">
<meta property="og:site_name" content="愚人而已">
<meta property="og:description" content="内容参考：  《深度学习入门：基于 Python 的理论与实现》(斋藤康毅) 上述书籍作者提供的代码  之前实现了神经网络的前向传播过程，但是用到的权重参数是预先准备好的，那么接下来的目标自然就是“学习”——从训练数据中自动获取最优权重参数。 从数据中学习机器学习是以数据为核心的，尝试从数据中发现答案和模式。神经网络或深度学习则比以往的机器学习方法更能避免人为介入。 要理解上面这些描述还是要从具体">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2022/10/08/jr6XuH83fihJRdU.png">
<meta property="og:image" content="https://s2.loli.net/2022/10/09/3dHoWFkPCrpvBGh.png">
<meta property="og:image" content="https://s2.loli.net/2022/10/14/XjxmfqakndbtHrc.png">
<meta property="og:image" content="https://s2.loli.net/2022/10/15/jE2J9w5pGhmbWfX.png">
<meta property="article:published_time" content="2022-10-07T15:18:35.000Z">
<meta property="article:modified_time" content="2022-11-07T08:43:40.831Z">
<meta property="article:author" content="零歌">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="神经网络">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2022/10/08/jr6XuH83fihJRdU.png"><script>(function() {
  if (CONFIG.mode !== 'auto') return
  const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches
  const setting = localStorage.getItem('darken-mode') || 'auto'
  if (setting === 'dark' || (prefersDark && setting !== 'light'))
    document.documentElement.classList.toggle('dark', true)
})()</script></head><body><script src="https://code.iconify.design/2/2.1.1/iconify.min.js"></script><script>// Define global variable
IconifyProviders = {
  // Empty prefix: overwrite default API provider configuration
  '': {
    // Use custom API first, use Iconify public API as backup
    resources: [
        'https://api.iconify.design',
    ],
    // Wait for 1 second before switching API hosts
    rotate: 1000,
  },
};</script><script defer src="https://fastly.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js" type="module"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js" type="module"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="文章目录"><span class="icon iconify" data-icon="ri:list-ordered"></span></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><span class="icon iconify" data-icon="ri:passport-line"></span></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="零歌"><img width="96" loading="lazy" src="/images/nekosense.jpeg" alt="零歌"></a><div class="site-author-name"><a href="/about/">零歌</a></div><span class="site-name">愚人而已</span><sub class="site-subtitle">以愚者之名攀上顶峰</sub><div class="site-description"></div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="首页"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:home-4-line"></span></span></a><div class="site-state-item"><a href="/archives/" title="归档"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:archive-line"></span></span><span class="site-state-item-count">45</span></a></div><div class="site-state-item"><a href="/categories/" title="分类"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:folder-2-line"></span></span><span class="site-state-item-count">5</span></a></div><div class="site-state-item"><a href="/tags/" title="标签"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="site-state-item-count">33</span></a></div><a class="site-state-item hty-icon-button" target="_blank" rel="noopener" href="https://yun.yunyoujun.cn" title="文档"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:settings-line"></span></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="/atom.xml" title="RSS" target="_blank" style="color:orange"><span class="icon iconify" data-icon="ri:rss-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/charliedu2000" title="GitHub" target="_blank" style="color:#6e5494"><span class="icon iconify" data-icon="ri:github-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://music.163.com/#/user/home?id=610609902" title="网易云音乐" target="_blank" style="color:#C20C0C"><span class="icon iconify" data-icon="ri:netease-cloud-music-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://space.bilibili.com/22660607" title="哔哩哔哩" target="_blank" style="color:#FF8EB3"><span class="icon iconify" data-icon="ri:bilibili-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="mailto:charliedu2000@hotmail.com" title="E-Mail" target="_blank" style="color:#8E71C1"><span class="icon iconify" data-icon="ri:mail-line"></span></a></div><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="旁友们" style="color:dodgerblue"><span class="icon iconify" data-icon="ri:genderless-line"></span></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><span class="icon iconify" data-icon="ri:contrast-2-line"></span></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8E%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.</span> <span class="toc-text">从数据中学习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E4%BA%9B%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9%EF%BC%9A-%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E5%92%8C%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE"><span class="toc-number">1.1.</span> <span class="toc-text">一些注意事项： 训练数据和测试数据</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">2.</span> <span class="toc-text">损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE"><span class="toc-number">2.1.</span> <span class="toc-text">均方误差</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E8%AF%AF%E5%B7%AE"><span class="toc-number">2.2.</span> <span class="toc-text">交叉熵误差</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mini-batch-%E5%AD%A6%E4%B9%A0"><span class="toc-number">2.3.</span> <span class="toc-text">mini-batch 学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">2.4.</span> <span class="toc-text">为什么需要损失函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E5%80%BC%E5%BE%AE%E5%88%86"><span class="toc-number">3.</span> <span class="toc-text">数值微分</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%BC%E6%95%B0"><span class="toc-number">3.1.</span> <span class="toc-text">导数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%81%8F%E5%AF%BC%E6%95%B0"><span class="toc-number">3.2.</span> <span class="toc-text">偏导数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6"><span class="toc-number">4.</span> <span class="toc-text">梯度</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E6%B3%95"><span class="toc-number">4.1.</span> <span class="toc-text">梯度法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%A2%AF%E5%BA%A6"><span class="toc-number">4.2.</span> <span class="toc-text">神经网络的梯度</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95"><span class="toc-number">5.</span> <span class="toc-text">实现学习算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%B1%BB"><span class="toc-number">5.1.</span> <span class="toc-text">2层神经网络的类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mini-batch-%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">5.2.</span> <span class="toc-text">mini-batch 的实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE%E7%9A%84%E8%AF%84%E4%BB%B7"><span class="toc-number">5.3.</span> <span class="toc-text">基于测试数据的评价</span></a></li></ol></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article" style="--smc-primary:#0078E7;"><link itemprop="mainEntityOfPage" href="https://afool.top/learning/deep-learning-from-scratch-learning-of-neural-network/"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="零歌"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="愚人而已"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">深度学习入门：神经网络的学习</h1><div class="post-meta"><div class="post-time" style="display:block"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-line"></span></span> <time title="创建时间：2022-10-07 23:18:35" itemprop="dateCreated datePublished" datetime="2022-10-07T23:18:35+08:00">2022-10-07</time></div><span class="post-count"><span class="post-symbolcount"><span class="post-meta-item-icon" title="本文字数"><span class="icon iconify" data-icon="ri:file-word-line"></span></span> <span title="本文字数">6.2k</span><span class="post-meta-divider">-</span><span class="post-meta-item-icon" title="阅读时长"><span class="icon iconify" data-icon="ri:timer-line"></span></span> <span title="阅读时长">24m</span></span></span><div class="post-classify"><span class="post-category"> <span class="post-meta-item-icon" style="margin-right:3px;"><span class="icon iconify" data-icon="ri:folder-line"></span></span><span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category-item" href="/categories/%E5%BF%AB%E5%8E%BB%E5%AD%A6%E4%B9%A0/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">快去学习</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag-item" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="tag-name">深度学习</span></a><a class="tag-item" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="tag-name">神经网络</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body"><p>内容参考：</p>
<ul>
<li>《深度学习入门：基于 Python 的理论与实现》(斋藤康毅)</li>
<li><a target="_blank" rel="noopener" href="https://github.com/oreilly-japan/deep-learning-from-scratch">上述书籍作者提供的代码</a></li>
</ul>
<p>之前实现了神经网络的前向传播过程，但是用到的权重参数是预先准备好的，那么接下来的目标自然就是“学习”——从训练数据中自动获取最优权重参数。</p>
<h2 id="从数据中学习"><a href="#从数据中学习" class="headerlink" title="从数据中学习"></a>从数据中学习</h2><p>机器学习是以数据为核心的，尝试从数据中发现答案和模式。<em>神经网络或深度学习则比以往的机器学习方法更能避免人为介入</em>。</p>
<p>要理解上面这些描述还是要从具体的问题开始，比如要实现数字“5”的识别该用什么算法呢？人很容易认出来眼前的是不是5，这可以说是基于经验和某些规律得出的结果，但是机器怎么知道这些经验或者规律是什么呢？与其想办法用算法表示这些规律（显然很难），不如想办法让机器从数据中直接获取需要的“规律”。现有的一种方案是先从图像中提取<strong>特征量</strong>，再用机器学习来学习这些特征量的模式。</p>
<blockquote>
<p>这里所说的“特征量”是指可以从输入数据（输入图像）中准确地提取本质数据（重要的数据）的转换器。图像的特征量通常表示为向量的形式。在计算机视觉领域，常用的特征量包括SIFT、SURF和HOG等。使用这些特征量将图像数据转换为向量，然后对转换后的向量使用机器学习中的SVM、KNN等分类器进行学习。</p>
</blockquote>
<p>在上面所说的方法中，将图像转换为向量时所用的特征量仍是人为设计的，对于不同的问题需要考虑不同的特征量。在深度学习中，这里所说的特征量也是由机器来学习的。</p>
<p><img src="https://s2.loli.net/2022/10/08/jr6XuH83fihJRdU.png" alt="从人工设计规则转变为由机器从数据中学习" loading="lazy"></p>
<p>书上给出的这幅图说明了识别的方法，从这里也能看出所谓“机器学习”和“深度学习”之间的一些区别。</p>
<blockquote>
<p>深度学习有时也称为端到端机器学习（end-to-end machine learning）。这里所说的端到端是指从一端到另一端的意思，也就是从原始数据（输入）中获得目标结果（输出）的意思。</p>
</blockquote>
<p><del>就是说中间的过程不需要人为干预吗……</del></p>
<h3 id="一些注意事项：-训练数据和测试数据"><a href="#一些注意事项：-训练数据和测试数据" class="headerlink" title="一些注意事项： 训练数据和测试数据"></a>一些注意事项： 训练数据和测试数据</h3><p>很明显，<strong>训练数据</strong>就是用来学习、寻找最优参数所用的那部分数据，而<strong>测试数据</strong>（监督数据）的作用是评价模型的泛化能力，自然不可缺少。如果只用已有的数据来学习和评价，可能会造成模型无法正确处理其他数据集，也就是出现<strong>过拟合</strong>现象。</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p><strong>损失函数</strong>可以表示神经网络的“状态”，神经网络就参考这个状态指标来寻找最优权重参数。这里说的损失函数表示的“状态”是指当前神经网络神经网络对监督数据在多大程度上不拟合、不一致，或者说性能有多坏（当然，为损失函数乘上一个负值就可以表示性能有多好）。损失函数一般用均方误差和交叉熵误差等。</p>
<h3 id="均方误差"><a href="#均方误差" class="headerlink" title="均方误差"></a>均方误差</h3><p>均方误差的公式如下：</p>
<p>$$ E &#x3D; \frac{1}{2} \sum \limits_k (y_k - t_k)^2 $$</p>
<p>其中 $y_k$ 表示神经网络的输出，$t_k$ 表示监督数据，$k$ 表示数据的维数。比方说在之前的手写数字识别的例子中，$y_k$、$t_k$ 就是由10个元素构成的数据：</p>
<pre class="language-pycon" data-language="pycon"><code class="language-pycon">&gt;&gt;&gt; y &#x3D; [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
&gt;&gt;&gt; t &#x3D; [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]</code></pre>

<p><code>y</code> 是 <code>softmax</code> 函数的输出，<code>t</code> 是监督数据的独热编码形式，均方误差会计算 <code>y</code> 和 <code>t</code> 的各个对应元素之差的平方，再求总和。实现方式可以是：</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># 均方误差</span>
<span class="token keyword">def</span> <span class="token function">mean_squared_error</span><span class="token punctuation">(</span>y<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token number">0.5</span> <span class="token operator">*</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">(</span>y <span class="token operator">-</span> t<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span></code></pre>

<p>均方误差得出的结果越小，说明结果与监督数据之间的误差越小。</p>
<h3 id="交叉熵误差"><a href="#交叉熵误差" class="headerlink" title="交叉熵误差"></a>交叉熵误差</h3><p>公式如下：</p>
<p>$$ E &#x3D; - \sum \limits_k t_k \log y_k $$</p>
<p>其中 $\log$ 表示 $\log_e$，$y_k$ 是神经网络的输出，$t_k$ 是正确解的标签（独热编码）。显然上式只会计算正确解标签的输出的自然对数（因为只有正确解标签的索引为1），也就是说交叉熵误差的值是由正确解标签所对应的输出结果决定的。</p>
<p>自然对数 $y &#x3D; \log x$ 的图像如下：</p>
<p><img src="https://s2.loli.net/2022/10/09/3dHoWFkPCrpvBGh.png" alt="自然对数 y = log x 的图像" loading="lazy"></p>
<p>那么可以看出正确解标签对应的输出为1时，交叉熵误差的输出为0，输出越小，交叉熵误差的结果越大。</p>
<p>实现方式：</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># 交叉熵误差</span>
<span class="token keyword">def</span> <span class="token function">cross_entropy_error</span><span class="token punctuation">(</span>y<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">:</span>
    delta <span class="token operator">=</span> <span class="token number">1e-7</span>
    <span class="token keyword">return</span> <span class="token operator">-</span>np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>t <span class="token operator">*</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span>y <span class="token operator">+</span> delta<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 防止出现 log(0) 无限大</span></code></pre>

<p>计算例子：</p>
<pre class="language-pycon" data-language="pycon"><code class="language-pycon">&gt;&gt;&gt; t &#x3D; [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
&gt;&gt;&gt; y &#x3D; [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
&gt;&gt;&gt; cross_entropy_error(np.array(y), np.array(t))
0.51082545709933802
&gt;&gt;&gt;
&gt;&gt;&gt; y &#x3D; [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
&gt;&gt;&gt; cross_entropy_error(np.array(y), np.array(t))
2.3025840929945458</code></pre>

<h3 id="mini-batch-学习"><a href="#mini-batch-学习" class="headerlink" title="mini-batch 学习"></a>mini-batch 学习</h3><p>所谓使用训练数据进行学习，就是针对训练数据计算损失函数的值，找出使该值尽可能小的参数。计算损失函数时必须将所有的训练数据作为对象，考虑所有训练数据的损失函数的总和。</p>
<p>以交叉熵误差为例，如果要计算所有训练数据的损失函数的总和，可以这样：</p>
<p>$$ E &#x3D; - \frac{1}{N} \sum \limits_n \sum \limits_k t_{nk} \log y_{nk} $$</p>
<p>N 自然是数据个数，$y_{nk}$ 是神经网络的输出中第 n 个数据的第 k 个元素的值，$t_{nk}$ 则是对应的监督数据。观察可以发现这里其实就是把单个数据的损失函数的公式扩大到了 N 个数据，随后除以 N 正规化处理。这个式子也可以说是“平均损失函数”。</p>
<p>在数据集比较大的情况下（比如之前用到的 MNIST 数据集，训练数据就有60000个），根据所有的训练数据来算损失函数的和代价就太大了。实际操作中一般是从全部数据中选出一部分作为全部数据的“近似”（<strong>mini-batch</strong>），用这些数据进行学习（mini-batch 学习）。比方说从60000个训练数据中随机选择100笔，在用这100笔数据进行学习。</p>
<p>（按我的理解，书上这里的说法应该是用样本代表整体。搜到的比较多的说法是将一批所有数据再分成若干份，称作 mini-batch，训练时一次更新一个 mini-batch，整个数据集会更新多次，最终还是使用了所有数据。）</p>
<p>随机抽取数据时可以利用 <code>np.random.choice()</code>，像这样：</p>
<pre class="language-python" data-language="python"><code class="language-python">train_size <span class="token operator">=</span> x_train<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>  <span class="token comment"># 对于 MNIST 的训练数据，是60000</span>
batch_size <span class="token operator">=</span> <span class="token number">10</span>
batch_mask <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>choice<span class="token punctuation">(</span>train_size<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>  <span class="token comment"># 在0到59999之间随机选择10个数，得到包含被选数据的索引的数组</span>
x_batch <span class="token operator">=</span> x_train<span class="token punctuation">[</span>batch_mask<span class="token punctuation">]</span>  <span class="token comment"># 被选测试数据</span>
t_batch <span class="token operator">=</span> t_train<span class="token punctuation">[</span>batch_mask<span class="token punctuation">]</span>  <span class="token comment"># 被选测试数据的标签</span></code></pre>

<p>如果要实现 mini-batch 的交叉熵误差，可以这样：</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">cross_entropy_error</span><span class="token punctuation">(</span>y<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 只有一个数据的时候改变数据形状</span>
    <span class="token keyword">if</span> y<span class="token punctuation">.</span>ndim <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
        t <span class="token operator">=</span> t<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> t<span class="token punctuation">.</span>size<span class="token punctuation">)</span>
        y <span class="token operator">=</span> y<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>size<span class="token punctuation">)</span>
 
    batch_size <span class="token operator">=</span> y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    <span class="token comment"># 如果监督数据不是独热形式而是正常的标签表示</span>
    <span class="token comment"># return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size</span>
    <span class="token keyword">return</span> <span class="token operator">-</span>np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>t <span class="token operator">*</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span>y <span class="token operator">+</span> <span class="token number">1e-7</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> batch_size</code></pre>

<p>应该可以根据独热编码1值的索引拿到对应标签，这样不论监督数据是哪种形式都可以写在一个实现里。</p>
<h3 id="为什么需要损失函数"><a href="#为什么需要损失函数" class="headerlink" title="为什么需要损失函数"></a>为什么需要损失函数</h3><p>按说学习的目标是使神经网络的识别精度足够高，那么用识别精度作为评判标准就好了，为什么还需要损失函数呢？</p>
<p>从前面的内容可以看出，寻找最优参数其实也是寻找使损失函数值尽可能小的参数的过程，为了找到损失函数的“最低点”，需要算出参数的导数（准确地说是梯度），根据导数反映出的变化趋势继续更新参数（对权重参数的损失函数求导，就表示如果稍微改变权重参数，损失函数将如何变化）。而识别精度很明显是离散值，很多时候微调参数后识别精度也不变化，这就导致在很多地方参数的导数会变为0，无法反映识别精度的变化趋势，也就无法继续进行神经网络的学习。</p>
<p>在激活函数上也是同样的道理，阶跃函数只在某个地方突变，其他地方导数都是0，无法体现参数改变带来的变化。而 sigmoid 函数的输出和曲线斜率（导数）都是连续变化的（且导数始终不为0），能保证神经网络的学习正确进行。</p>
<p><del>暂且记下，更“深”的理解可能还要我接着学吧……</del></p>
<h2 id="数值微分"><a href="#数值微分" class="headerlink" title="数值微分"></a>数值微分</h2><p><del>这里引入数值微分的方法，用它计算导数和偏导数。</del></p>
<h3 id="导数"><a href="#导数" class="headerlink" title="导数"></a>导数</h3><p>导数的概念应该不用多说，就算高数渣如我也应该有所认识……简单来说就是“瞬间的变化量”。</p>
<p>求函数的导数：</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">numerical_diff</span><span class="token punctuation">(</span>f<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    h <span class="token operator">=</span> <span class="token number">1e-4</span> <span class="token comment"># 0.0001，避免使用过小的值</span>
    <span class="token keyword">return</span> <span class="token punctuation">(</span>f<span class="token punctuation">(</span>x<span class="token operator">+</span>h<span class="token punctuation">)</span> <span class="token operator">-</span> f<span class="token punctuation">(</span>x<span class="token operator">-</span>h<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token operator">*</span>h<span class="token punctuation">)</span> <span class="token comment"># 使用中心差分来计算</span></code></pre>

<p>使用例，有函数 $f(x) &#x3D; 0.01 x^2 + 0.1 x$：</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">function_1</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token number">0.01</span><span class="token operator">*</span>x<span class="token operator">**</span><span class="token number">2</span> <span class="token operator">+</span> <span class="token number">0.1</span><span class="token operator">*</span>x  <span class="token comment"># 0.01x^2 + 0.1x</span></code></pre>

<p>求其在 $x&#x3D;x_0$ 处的导数：</p>
<pre class="language-pycon" data-language="pycon"><code class="language-pycon">&gt;&gt;&gt; numerical_diff(function_1, x0)</code></pre>

<blockquote>
<p>利用微小的差分求导数的过程称为数值微分，而基于数学式的推导求导数的过程，则用“解析性”（analytic）一词，称为“解析性求解”或者“解析性求导”。</p>
</blockquote>
<h3 id="偏导数"><a href="#偏导数" class="headerlink" title="偏导数"></a>偏导数</h3><p>一个多变量的函数，关于其中一个变量求导数，其他变量恒定。<del>应该不用再解释……</del></p>
<p>计算偏导的例子，有函数 $f(x_0, x_1) &#x3D; x_0^2 + x_1^2$：</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">function_2</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>x<span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">)</span></code></pre>

<p>求 $x_0 &#x3D; 3$，$x_1 &#x3D; 4$ 时，关于 $x_0$ 的偏导数 $\frac{\partial f}{\partial x_0}$：</p>
<pre class="language-pycon" data-language="pycon"><code class="language-pycon">&gt;&gt;&gt; def function_tmp1(x0):
...     return x0*x0 + 4.0**2.0
...
&gt;&gt;&gt; numerical_diff(function_tmp1, 3.0)</code></pre>

<p><del>求关于 $x_1$ 的偏导数也是差不多的方法。</del></p>
<h2 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h2><p>前面按变量分别求偏导，如果把全部变量的偏导数汇总成向量，就是<strong>梯度</strong>。求梯度的实现方法可以是这样，非批处理：</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># 求梯度</span>
<span class="token keyword">def</span> <span class="token function">numerical_gradient</span><span class="token punctuation">(</span>f<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    h <span class="token operator">=</span> <span class="token number">1e-4</span>  <span class="token comment"># 0.0001</span>
    grad <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># 生成和 x 形状相同、元素都为0的数组</span>

    <span class="token keyword">for</span> idx <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        tmp_val <span class="token operator">=</span> x<span class="token punctuation">[</span>idx<span class="token punctuation">]</span>
        <span class="token comment"># 计算 f(x+h)</span>
        x<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">=</span> tmp_val <span class="token operator">+</span> h
        fxh1 <span class="token operator">=</span> f<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        <span class="token comment"># 计算 f(x-h)</span>
        x<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">=</span> tmp_val <span class="token operator">-</span> h
        fxh2 <span class="token operator">=</span> f<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        <span class="token comment"># 算出偏导，还原 x</span>
        grad<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>fxh1 <span class="token operator">-</span> fxh2<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token operator">*</span>h<span class="token punctuation">)</span>
        x<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">=</span> tmp_val

    <span class="token keyword">return</span> grad</code></pre>

<p>这里求梯度用的方法其实和前面求单个变量的数值微分所用的方法没有什么区别……按照<a target="_blank" rel="noopener" href="https://github.com/oreilly-japan/deep-learning-from-scratch/blob/master/ch04/gradient_2d.py">作者给出的方法</a>（里面实现了批处理计算梯度）可以画出 $f(x_0, x_1) &#x3D; x_0^2 + x_1^2$ 的负梯度：</p>
<p><img src="https://s2.loli.net/2022/10/14/XjxmfqakndbtHrc.png" alt="函数的负梯度" loading="lazy"></p>
<p><del>函数图像可看做下凹的曲面，负梯度会指向最低点，更好理解梯度在这里的作用吧……</del>在这个图中，离函数最小值越远，梯度向量的模就越大（变化越快），且向量都指向了“最低处”。而一般来说，“梯度”指向该点处函数值减小最多的方向。</p>
<h3 id="梯度法"><a href="#梯度法" class="headerlink" title="梯度法"></a>梯度法</h3><p>神经网络需要在学习过程中找到最优参数（权重和偏置），也就是使损失函数取最小值时的参数。损失函数很复杂的时候就难以找到最小值所在的“点”，但是可以根据梯度表示的“变化快慢”来寻找尽可能小的值。不过梯度反映的是各点处函数值减小最多的方向，并不一定能找到最小值：梯度为0的点除了最小值还有极小值和鞍点。尽管如此，沿着梯度方向也能最大限度地减小函数值，所以可以沿着梯度方向寻找尽可能小的值。</p>
<blockquote>
<p>在梯度法中，函数的取值从当前位置沿着梯度方向前进一定距离，然后在新的地方重新求梯度，再沿着新梯度方向前进，如此反复，不断地沿梯度方向前进。像这样，通过不断地沿梯度方向前进，逐渐减小函数值的过程就是<strong>梯度法</strong>（gradient method）。梯度法是解决机器学习中最优化问题的常用方法，特别是在神经网络的学习中经常被使用。</p>
</blockquote>
<p>寻找最小值的梯度法叫做梯度下降法，寻找最大值的梯度法叫做梯度上升法。反转损失函数的符号，两类问题就可以互相转化。在神经网络中，梯度法主要指梯度下降法。</p>
<p>对于函数 $f(x_0, x_1) &#x3D; x_0^2 + x_1^2$，用数学式表示梯度法：</p>
<p>$$ x_0 &#x3D; x_0 - \eta \frac{\partial f}{\partial x_0} \\ \\ x_1 &#x3D; x_1 - \eta \frac{\partial f}{\partial x_1} $$</p>
<p>上式中 $\eta$ 表示更新量（<strong>学习率</strong>），它决定了在一次学习中应该学习多少以及应该在多大程度上更新参数。这组式子表示更新一次，学习时反复执行这个步骤减小函数值。变量数量增加后方法也是类似的。</p>
<p>学习率需要事先确定，过大或过小都无法取得理想的结果。在学习过程中一般会一边改变学习率，一边观察学习是否正确进行。</p>
<p>使用梯度下降法的例子（<code>numerical_gradient</code> 的实现参考<a target="_blank" rel="noopener" href="https://github.com/oreilly-japan/deep-learning-from-scratch/blob/master/ch04/gradient_2d.py">这里</a>）：</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> sys
<span class="token keyword">import</span> os

sys<span class="token punctuation">.</span>path<span class="token punctuation">.</span>append<span class="token punctuation">(</span>os<span class="token punctuation">.</span>curdir<span class="token punctuation">)</span>

<span class="token keyword">from</span> ch04<span class="token punctuation">.</span>gradient_2d <span class="token keyword">import</span> numerical_gradient


<span class="token keyword">def</span> <span class="token function">function_2</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">**</span><span class="token number">2</span> <span class="token operator">+</span> x<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">**</span><span class="token number">2</span>


<span class="token comment"># f: 函数</span>
<span class="token comment"># init_x: 初始值</span>
<span class="token comment"># lr: 学习率</span>
<span class="token comment"># step_num： 梯度法重复次数</span>
<span class="token keyword">def</span> <span class="token function">gradient_descent</span><span class="token punctuation">(</span>f<span class="token punctuation">,</span> init_x<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span> step_num<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    x <span class="token operator">=</span> init_x

    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>step_num<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 反复执行更新</span>
        grad <span class="token operator">=</span> numerical_gradient<span class="token punctuation">(</span>f<span class="token punctuation">,</span> x<span class="token punctuation">)</span>
        x <span class="token operator">-=</span> lr <span class="token operator">*</span> grad

    <span class="token keyword">return</span> x


init_x <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">3.0</span><span class="token punctuation">,</span> <span class="token number">4.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>gradient_descent<span class="token punctuation">(</span>function_2<span class="token punctuation">,</span> init_x<span class="token operator">=</span>init_x<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> step_num<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre>

<p>输出：</p>
<pre class="language-bash" data-language="bash"><code class="language-bash"><span class="token punctuation">[</span>-6.11110793e-10  <span class="token number">8</span>.14814391e-10<span class="token punctuation">]</span></code></pre>

<p>上面设初始值为 <code>(-3.0, 4.0)</code>，用梯度法寻找最小值，得到的结果很接近函数的最小值点 <code>(0, 0)</code>。如果学习率不合适，得到的结果就会偏离更远。事实上，学习率过大时结果会过度发散，学习率太小时参数更新幅度太小，也很难得到合适的结果。</p>
<blockquote>
<p>像学习率这样的参数称为超参数。这是一种和神经网络的参数（权重和偏置）性质不同的参数。相对于神经网络的权重参数是通过训练数据和学习算法自动获得的，学习率这样的超参数则是人工设定的。一般来说，超参数需要尝试多个值，以便找到一种可以使学习顺利进行的设定。</p>
</blockquote>
<h3 id="神经网络的梯度"><a href="#神经网络的梯度" class="headerlink" title="神经网络的梯度"></a>神经网络的梯度</h3><p>神经网络的学习需要用到损失函数关于权重参数的梯度。<del>在一定的输入下权重参数确实就是变量呢……数学渣的小心确认……</del>参考书上举的例子的话，对于一个只有一个形状为 $2 \times 3$ 的权重 $\boldsymbol{W}$ 的神经网络，损失函数用 $L$ 表示，梯度就可以用 $\frac{\partial L}{\partial \boldsymbol{W}}$ 表示。数学式如下：</p>
<p>$$ \boldsymbol{W} &#x3D; \begin{pmatrix} w_{1 1} &amp; w_{1 2} &amp; w_{1 3} \\ w_{2 1} &amp; w_{2 2} &amp; w_{2 3} \end{pmatrix} $$</p>
<p>$$ \frac{\partial L}{\partial \boldsymbol{W}} &#x3D; \begin{pmatrix} \frac{\partial L}{\partial w_{1 1}} &amp; \frac{\partial L}{\partial w_{1 2}} &amp; \frac{\partial L}{\partial w_{1 3}} \\ \frac{\partial L}{\partial w_{2 1}} &amp; \frac{\partial L}{\partial w_{2 2}} &amp; \frac{\partial L}{\partial w_{2 3}} \end{pmatrix} $$</p>
<p>$\frac{\partial L}{\partial \boldsymbol{W}}$ 的元素由各个元素对应的偏导数构成，形状与 $\boldsymbol{W}$ 相同。</p>
<p>就以这样一个神经网络为例，实现求梯度：</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> sys<span class="token punctuation">,</span> os
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

sys<span class="token punctuation">.</span>path<span class="token punctuation">.</span>append<span class="token punctuation">(</span>os<span class="token punctuation">.</span>curdir<span class="token punctuation">)</span>

<span class="token keyword">from</span> common<span class="token punctuation">.</span>func <span class="token keyword">import</span> softmax<span class="token punctuation">,</span> cross_entropy_error
<span class="token keyword">from</span> common<span class="token punctuation">.</span>gradient <span class="token keyword">import</span> numerical_gradient


<span class="token keyword">class</span> <span class="token class-name">simpleNet</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>W <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>  <span class="token comment"># 利用高斯分布随机生成0到1之间的数，填充指定形状的多维数组</span>

    <span class="token keyword">def</span> <span class="token function">predict</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>W<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">loss</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">:</span>
        z <span class="token operator">=</span> self<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        y <span class="token operator">=</span> softmax<span class="token punctuation">(</span>z<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> cross_entropy_error<span class="token punctuation">(</span>y<span class="token punctuation">,</span> t<span class="token punctuation">)</span>

        <span class="token keyword">return</span> loss


net <span class="token operator">=</span> simpleNet<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'net.W:\n'</span><span class="token punctuation">,</span> net<span class="token punctuation">.</span>W<span class="token punctuation">)</span>  <span class="token comment"># net 的权重参数</span>

x <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.6</span><span class="token punctuation">,</span> <span class="token number">0.9</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
p <span class="token operator">=</span> net<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'predict x:\n'</span><span class="token punctuation">,</span> p<span class="token punctuation">)</span>  <span class="token comment"># 输出</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'index of max value:\n'</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>p<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 最大值索引</span>

t <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'net.loss:\n'</span><span class="token punctuation">,</span> net<span class="token punctuation">.</span>loss<span class="token punctuation">(</span>x<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 计算交叉熵损失</span>

<span class="token comment"># W 是伪参数，计算梯度时会执行 f，用到的是 net 内的 W</span>
<span class="token comment"># def f(W):</span>
<span class="token comment">#     return net.loss(x, t)</span>
<span class="token comment"># 也可以用 lambda 表达式</span>
f <span class="token operator">=</span> <span class="token keyword">lambda</span> w<span class="token punctuation">:</span> net<span class="token punctuation">.</span>loss<span class="token punctuation">(</span>x<span class="token punctuation">,</span> t<span class="token punctuation">)</span>

dW <span class="token operator">=</span> numerical_gradient<span class="token punctuation">(</span>f<span class="token punctuation">,</span> net<span class="token punctuation">.</span>W<span class="token punctuation">)</span>  <span class="token comment"># 求关于权重的梯度，权重自然是 f 的参数</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'dW:\n'</span><span class="token punctuation">,</span> dW<span class="token punctuation">)</span></code></pre>

<p>某一次运行上面代码的输出：</p>
<pre class="language-bash" data-language="bash"><code class="language-bash">net.W:
 <span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.03900802</span> <span class="token parameter variable">-0.24126244</span> -1.60380801<span class="token punctuation">]</span>
 <span class="token punctuation">[</span>-0.94332177  <span class="token number">1.36421781</span> -1.05313682<span class="token punctuation">]</span><span class="token punctuation">]</span>
predict x:
 <span class="token punctuation">[</span>-0.82558478  <span class="token number">1.08303856</span> -1.91010794<span class="token punctuation">]</span>
index of max value:
 <span class="token number">1</span>
net.loss:
 <span class="token number">3.174142995517425</span>
dW:
 <span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.07424015</span>  <span class="token number">0.50066058</span> -0.57490072<span class="token punctuation">]</span>
 <span class="token punctuation">[</span> <span class="token number">0.11136022</span>  <span class="token number">0.75099087</span> -0.86235108<span class="token punctuation">]</span><span class="token punctuation">]</span></code></pre>

<p>计算 <code>dW</code> 得到的结果是与 <code>W</code> 形状一致的二维数组。看结果的话，例如 $\frac{\partial L}{\partial w_{1 2}}$ 的值约为 $0.5$，表示如果 $w_{1 2}$ 增加 $h$，损失函数的值会增加 $0.5 h$；再比方说 $\frac{\partial L}{\partial w_{2 3}}$ 的值约为 $-0.86$，表示如果 $w_{2 3}$ 增加 $h$，损失函数的值会减小 $0.86 h$。因为要减小损失函数的值，所以 $w_{1 2}$ 应该往负方向更新，$w_{2 3}$ 应该往正方向更新，且更新 $w_{2 3}$ 比更新 $w_{1 2}$ 效果更明显。</p>
<h2 id="实现学习算法"><a href="#实现学习算法" class="headerlink" title="实现学习算法"></a>实现学习算法</h2><blockquote>
<p>神经网络的学习步骤如下所示。</p>
<ul>
<li><strong>前提</strong><ul>
<li>神经网络存在合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为“学习”。神经网络的学习分成下面4个步骤。</li>
</ul>
</li>
<li><strong>步骤1（mini-batch）</strong><ul>
<li>从训练数据中随机选出一部分数据，这部分数据称为 mini-batch。我们的目标是减小 mini-batch 的损失函数的值。</li>
</ul>
</li>
<li><strong>步骤2（计算梯度）</strong><ul>
<li>为了减小 mini-batch 的损失函数的值，需要求出各个权重参数的梯度。梯度表示损失函数的值减小最多的方向。</li>
</ul>
</li>
<li><strong>步骤3（更新参数）</strong><ul>
<li>将权重参数沿梯度方向进行微小更新。</li>
</ul>
</li>
<li><strong>步骤4（重复）</strong><ul>
<li>重复步骤1、步骤2、步骤3</li>
</ul>
</li>
</ul>
</blockquote>
<p>上面的方法通过梯度下降更新参数，因为使用了随机选择的 mini-batch，所以又叫做<strong>随机梯度下降法</strong>（stochastic gradient descent，SGD）。</p>
<p>下面跟着书上的指导实现手写数字识别的神经网络。以2层神经网络（有1层隐藏层）为对象，使用 MNIST 数据集进行学习。</p>
<h3 id="2层神经网络的类"><a href="#2层神经网络的类" class="headerlink" title="2层神经网络的类"></a>2层神经网络的类</h3><p>为2层神经网络实现一个类，如下所示：</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> sys<span class="token punctuation">,</span> os
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

sys<span class="token punctuation">.</span>path<span class="token punctuation">.</span>append<span class="token punctuation">(</span>os<span class="token punctuation">.</span>curdir<span class="token punctuation">)</span>

<span class="token keyword">from</span> common<span class="token punctuation">.</span>func <span class="token keyword">import</span> <span class="token operator">*</span>
<span class="token keyword">from</span> common<span class="token punctuation">.</span>gradient <span class="token keyword">import</span> numerical_gradient


<span class="token keyword">class</span> <span class="token class-name">TwoLayerNet</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>
                 input_size<span class="token punctuation">,</span>
                 hidden_size<span class="token punctuation">,</span>
                 output_size<span class="token punctuation">,</span>
                 weight_init_std<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 初始化权重偏置，注意各层参数的形状</span>
        self<span class="token punctuation">.</span>params <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>
        self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'W1'</span><span class="token punctuation">]</span> <span class="token operator">=</span> weight_init_std <span class="token operator">*</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>
            input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'b1'</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>hidden_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'W2'</span><span class="token punctuation">]</span> <span class="token operator">=</span> weight_init_std <span class="token operator">*</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>
            hidden_size<span class="token punctuation">,</span> output_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'b2'</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>output_size<span class="token punctuation">)</span>

    <span class="token comment"># 推理过程</span>
    <span class="token keyword">def</span> <span class="token function">predict</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        W1<span class="token punctuation">,</span> W2 <span class="token operator">=</span> self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'W1'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'W2'</span><span class="token punctuation">]</span>
        b1<span class="token punctuation">,</span> b2 <span class="token operator">=</span> self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'b1'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'b2'</span><span class="token punctuation">]</span>

        a1 <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> W1<span class="token punctuation">)</span> <span class="token operator">+</span> b1
        z1 <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>a1<span class="token punctuation">)</span>
        a2 <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>z1<span class="token punctuation">,</span> W2<span class="token punctuation">)</span> <span class="token operator">+</span> b2
        y <span class="token operator">=</span> softmax<span class="token punctuation">(</span>a2<span class="token punctuation">)</span>

        <span class="token keyword">return</span> y

    <span class="token comment"># 计算损失函数的值</span>
    <span class="token comment"># x 是输入数据，t 是监督数据</span>
    <span class="token keyword">def</span> <span class="token function">loss</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">:</span>
        y <span class="token operator">=</span> self<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        <span class="token keyword">return</span> cross_entropy_error<span class="token punctuation">(</span>y<span class="token punctuation">,</span> t<span class="token punctuation">)</span>

    <span class="token comment"># 计算识别精度</span>
    <span class="token comment"># x 是输入数据，t 是监督数据</span>
    <span class="token keyword">def</span> <span class="token function">accuracy</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">:</span>
        y <span class="token operator">=</span> self<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># 获得输出</span>
        y <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>y<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 取得输出最大值对应索引（标签）</span>
        t <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>t<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

        accuracy <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>y <span class="token operator">==</span> t<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token builtin">float</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> accuracy

    <span class="token comment"># 求梯度</span>
    <span class="token comment"># x 是输入数据，t 是监督数据</span>
    <span class="token keyword">def</span> <span class="token function">numerical_gradient</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">:</span>
        loss_W <span class="token operator">=</span> <span class="token keyword">lambda</span> W<span class="token punctuation">:</span> self<span class="token punctuation">.</span>loss<span class="token punctuation">(</span>x<span class="token punctuation">,</span> t<span class="token punctuation">)</span>

        <span class="token comment"># 对每层的参数求梯度</span>
        grads <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>
        grads<span class="token punctuation">[</span><span class="token string">'W1'</span><span class="token punctuation">]</span> <span class="token operator">=</span> numerical_gradient<span class="token punctuation">(</span>loss_W<span class="token punctuation">,</span> self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'W1'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        grads<span class="token punctuation">[</span><span class="token string">'b1'</span><span class="token punctuation">]</span> <span class="token operator">=</span> numerical_gradient<span class="token punctuation">(</span>loss_W<span class="token punctuation">,</span> self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'b1'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        grads<span class="token punctuation">[</span><span class="token string">'W2'</span><span class="token punctuation">]</span> <span class="token operator">=</span> numerical_gradient<span class="token punctuation">(</span>loss_W<span class="token punctuation">,</span> self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'W2'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        grads<span class="token punctuation">[</span><span class="token string">'b2'</span><span class="token punctuation">]</span> <span class="token operator">=</span> numerical_gradient<span class="token punctuation">(</span>loss_W<span class="token punctuation">,</span> self<span class="token punctuation">.</span>params<span class="token punctuation">[</span><span class="token string">'b2'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> grads</code></pre>

<p>（<code>weight_init_std</code> 似乎是为了解决激活后分布集中在0和1附近？）</p>
<p>之前也提到过，根据数据集和要识别的目标，<code>input_size</code> 就是<code>784</code>，<code>output_size</code> 是<code>10</code>。这里的隐藏层神经元个数设置为一个合理的值就行。</p>
<h3 id="mini-batch-的实现"><a href="#mini-batch-的实现" class="headerlink" title="mini-batch 的实现"></a>mini-batch 的实现</h3><p>实现过程如下：</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> sys<span class="token punctuation">,</span> os
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

sys<span class="token punctuation">.</span>path<span class="token punctuation">.</span>append<span class="token punctuation">(</span>os<span class="token punctuation">.</span>curdir<span class="token punctuation">)</span>

<span class="token keyword">from</span> ch04<span class="token punctuation">.</span>two_layer_net <span class="token keyword">import</span> TwoLayerNet
<span class="token keyword">from</span> dataset<span class="token punctuation">.</span>mnist <span class="token keyword">import</span> load_mnist

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Load mnist dataset...'</span><span class="token punctuation">)</span>
<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> t_train<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> t_test<span class="token punctuation">)</span> <span class="token operator">=</span> load_mnist<span class="token punctuation">(</span>normalize<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                                  one_hot_label<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

train_loss_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

<span class="token comment"># 超参数</span>
iters_num <span class="token operator">=</span> <span class="token number">10000</span>  <span class="token comment"># 梯度法更新次数</span>
train_size <span class="token operator">=</span> x_train<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>  <span class="token comment"># 训练集大小</span>
batch_size <span class="token operator">=</span> <span class="token number">100</span>  <span class="token comment"># batch 大小</span>
learning_rate <span class="token operator">=</span> <span class="token number">0.1</span>  <span class="token comment"># 学习率</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Initialize network...'</span><span class="token punctuation">)</span>
network <span class="token operator">=</span> TwoLayerNet<span class="token punctuation">(</span>input_size<span class="token operator">=</span><span class="token number">784</span><span class="token punctuation">,</span> hidden_size<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span> output_size<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>iters_num<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 从训练数据中随机获取 mini-batch</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>i<span class="token punctuation">,</span> <span class="token string">': choose mini-batch...'</span><span class="token punctuation">)</span>
    batch_mask <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>choice<span class="token punctuation">(</span>train_size<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>
    x_batch <span class="token operator">=</span> x_train<span class="token punctuation">[</span>batch_mask<span class="token punctuation">]</span>
    t_batch <span class="token operator">=</span> t_train<span class="token punctuation">[</span>batch_mask<span class="token punctuation">]</span>

    <span class="token comment"># 计算梯度</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>i<span class="token punctuation">,</span> <span class="token string">': calculate grads...'</span><span class="token punctuation">)</span>
    grad <span class="token operator">=</span> network<span class="token punctuation">.</span>numerical_gradient<span class="token punctuation">(</span>x_batch<span class="token punctuation">,</span> t_batch<span class="token punctuation">)</span>

    <span class="token comment"># 更新参数</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>i<span class="token punctuation">,</span> <span class="token string">': update params...'</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> key <span class="token keyword">in</span> <span class="token punctuation">(</span><span class="token string">'W1'</span><span class="token punctuation">,</span> <span class="token string">'b1'</span><span class="token punctuation">,</span> <span class="token string">'W2'</span><span class="token punctuation">,</span> <span class="token string">'b2'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        network<span class="token punctuation">.</span>params<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">-=</span> learning_rate <span class="token operator">*</span> grad<span class="token punctuation">[</span>key<span class="token punctuation">]</span>

    <span class="token comment"># 记录学习过程</span>
    loss <span class="token operator">=</span> network<span class="token punctuation">.</span>loss<span class="token punctuation">(</span>x_batch<span class="token punctuation">,</span> t_batch<span class="token punctuation">)</span>
    train_loss_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">)</span></code></pre>

<p>上面的学习过程应该算很清楚了，共更新10000次参数，会记录每一次训练后的损失函数值。</p>
<h3 id="基于测试数据的评价"><a href="#基于测试数据的评价" class="headerlink" title="基于测试数据的评价"></a>基于测试数据的评价</h3><p>上面的学习过程得到的损失函数值其实是对训练数据的某个 mini-batch 的损失函数值，这个值减小说明学习过程确实在正常进行，但是不能保证神经网络能正确识别训练集以外的数据。要评价神经网络的泛化能力，就要使用不在训练集中的数据。</p>
<p>这里就涉及了 <strong>epoch</strong>（之前做比赛看队友的代码里出现过这个东西，不过当时不知道具体表示什么意思），一个 epoch 表示学习中所有训练数据均被使用过一次时的更新次数。比如对于10000笔训练数据，如果 mini-batch 的大小是100，那么重复梯度下降100次，所有的训练数据就都被使用过一次，此时100次就是一个 epoch。（一般的做法是把训练数据打乱，根据指定的批大小生成 mini-batch，遍历所有 mini-batch 完成一个 epoch。像前面直接每次随机选择不能保证每个数据都被用到。）</p>
<p>在学习过程中需要定期对训练数据和测试数据记录识别精度，这里每经过一个 epoch 就记录一次：</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> sys<span class="token punctuation">,</span> os
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

sys<span class="token punctuation">.</span>path<span class="token punctuation">.</span>append<span class="token punctuation">(</span>os<span class="token punctuation">.</span>curdir<span class="token punctuation">)</span>

<span class="token keyword">from</span> ch04<span class="token punctuation">.</span>two_layer_net <span class="token keyword">import</span> TwoLayerNet
<span class="token keyword">from</span> dataset<span class="token punctuation">.</span>mnist <span class="token keyword">import</span> load_mnist

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Load mnist dataset...'</span><span class="token punctuation">)</span>
<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> t_train<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> t_test<span class="token punctuation">)</span> <span class="token operator">=</span> load_mnist<span class="token punctuation">(</span>normalize<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                                  one_hot_label<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Initialize network...'</span><span class="token punctuation">)</span>
network <span class="token operator">=</span> TwoLayerNet<span class="token punctuation">(</span>input_size<span class="token operator">=</span><span class="token number">784</span><span class="token punctuation">,</span> hidden_size<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span> output_size<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span>

<span class="token comment"># 超参数</span>
iters_num <span class="token operator">=</span> <span class="token number">10000</span>  <span class="token comment"># 梯度法更新次数</span>
train_size <span class="token operator">=</span> x_train<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>  <span class="token comment"># 训练集大小</span>
batch_size <span class="token operator">=</span> <span class="token number">100</span>  <span class="token comment"># batch 大小</span>
learning_rate <span class="token operator">=</span> <span class="token number">0.1</span>  <span class="token comment"># 学习率</span>

train_loss_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
train_acc_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
test_acc_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

<span class="token comment"># 平均每个 epoch 的重复次数</span>
iter_per_epoch <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span>train_size <span class="token operator">/</span> batch_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>iters_num<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 获取 mini-batch</span>
    batch_mask <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>choice<span class="token punctuation">(</span>train_size<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>
    x_batch <span class="token operator">=</span> x_train<span class="token punctuation">[</span>batch_mask<span class="token punctuation">]</span>
    t_batch <span class="token operator">=</span> t_train<span class="token punctuation">[</span>batch_mask<span class="token punctuation">]</span>

    <span class="token comment"># 计算梯度</span>
    grad <span class="token operator">=</span> network<span class="token punctuation">.</span>numerical_gradient<span class="token punctuation">(</span>x_batch<span class="token punctuation">,</span> t_batch<span class="token punctuation">)</span>

    <span class="token comment"># 更新参数</span>
    <span class="token keyword">for</span> key <span class="token keyword">in</span> <span class="token punctuation">(</span><span class="token string">'W1'</span><span class="token punctuation">,</span> <span class="token string">'b1'</span><span class="token punctuation">,</span> <span class="token string">'W2'</span><span class="token punctuation">,</span> <span class="token string">'b2'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        network<span class="token punctuation">.</span>params<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">-=</span> learning_rate <span class="token operator">*</span> grad<span class="token punctuation">[</span>key<span class="token punctuation">]</span>

    <span class="token comment"># 记录学习过程</span>
    loss <span class="token operator">=</span> network<span class="token punctuation">.</span>loss<span class="token punctuation">(</span>x_batch<span class="token punctuation">,</span> t_batch<span class="token punctuation">)</span>
    train_loss_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>

    <span class="token comment"># 每个 epoch 完成后计算识别精度</span>
    <span class="token keyword">if</span> i <span class="token operator">%</span> iter_per_epoch <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
        train_acc <span class="token operator">=</span> network<span class="token punctuation">.</span>accuracy<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> t_train<span class="token punctuation">)</span>
        test_acc <span class="token operator">=</span> network<span class="token punctuation">.</span>accuracy<span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> t_test<span class="token punctuation">)</span>
        train_acc_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>train_acc<span class="token punctuation">)</span>
        test_acc_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>test_acc<span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Train acc, test acc | '</span> <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span>train_acc<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">','</span> <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span>test_acc<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre>

<p>画出识别精度关于 epoch 的曲线：</p>
<p><img src="https://s2.loli.net/2022/10/15/jE2J9w5pGhmbWfX.png" alt="识别精度" loading="lazy"></p>
<p>随着学习进行，使用训练数据和测试数据评价的识别精度都在提高，两者几乎没有差距，没有发生过拟合。</p>
<p><del>这一部分就到这里……</del></p>
<p>之前写过的 softmax 函数似乎要换成这样的实现，可能和多维数组有关（具体到这里就是二维的吧，因为数据都是 mini-batch 了，max 和 sum 都需要沿着第2维），不过是不是我还得想想……</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">softmax</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    x <span class="token operator">=</span> x <span class="token operator">-</span> np<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> keepdims<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token comment"># 防止 exp() 溢出</span>
    <span class="token keyword">return</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token operator">/</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> keepdims<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span></code></pre>
</div></section><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>零歌</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="https://afool.top/learning/deep-learning-from-scratch-learning-of-neural-network/" title="深度学习入门：神经网络的学习">https://afool.top/learning/deep-learning-from-scratch-learning-of-neural-network/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><span class="icon iconify" data-icon="ri:creative-commons-line"></span><span class="icon iconify" data-icon="ri:creative-commons-by-line"></span><span class="icon iconify" data-icon="ri:creative-commons-nc-line"></span><span class="icon iconify" data-icon="ri:creative-commons-sa-line"></span></a> 许可协议。</li></ul></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/learning/deep-learning-from-scratch-error-back-propagation/" rel="prev" title="深度学习入门：误差反向传播法"><span class="icon iconify" data-icon="ri:arrow-left-s-line"></span><span class="post-nav-text">深度学习入门：误差反向传播法</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/learning/deep-learning-from-scratch-neural-network/" rel="next" title="深度学习入门：神经网络"><span class="post-nav-text">深度学习入门：神经网络</span><span class="icon iconify" data-icon="ri:arrow-right-s-line"></span></a></div></div></div><div class="hty-card" id="comment"><div class="comment-tooltip text-center"><span>留个爪印吧~</span><br></div><div id="tcomment"></div><script type="module">import { getScript } from '/js/utils.js'

getScript("https://fastly.jsdelivr.net/npm/twikoo@latest/dist/twikoo.all.min.js", () => {
  const twikooConfig = {"enable":true,"envId":"https://twikoo-afool.vercel.app/"}
  twikooConfig.el = '#tcomment'
  twikoo.init(twikooConfig)
}, window.twikoo);</script></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2020 – 2022 </span><span class="with-love" id="animate"><span class="icon iconify" data-icon="ri:cloud-line"></span></span><span class="author"> 零歌</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v6.3.0</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.10.9</span></div></footer></div><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><span class="icon iconify" data-icon="ri:arrow-up-s-line"></span><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a></body></html>