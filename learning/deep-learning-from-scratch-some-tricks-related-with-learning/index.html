<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="零歌"><meta name="copyright" content="零歌"><meta name="generator" content="Hexo 6.3.0"><meta name="theme" content="hexo-theme-yun"><title>深度学习入门：与学习相关的技巧 | 愚人而已</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/star-markdown-css@0.4.1/dist/yun/yun-markdown.min.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/prism-theme-vars/base.css"><script src="https://fastly.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>function initScrollReveal() {
  [".post-card",".markdown-body img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
}
document.addEventListener("DOMContentLoaded", initScrollReveal);
document.addEventListener("pjax:success", initScrollReveal);
</script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"></script><script type="module">import { renderKatex } from '/js/utils.js'
document.addEventListener("DOMContentLoaded", () => {
  renderKatex({
    ...{},
    ...true?.options,
  });
});</script><link rel="icon" type="image/svg+xml" href="/afool.svg"><link rel="mask-icon" href="/afool.svg" color="#0078E7"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="preconnect" href="https://fastly.jsdelivr.net/npm/" crossorigin><script id="yun-config">
    window.Yun = {}
    window.CONFIG = {"hostname":"afool.top","root":"/","title":"愚人而已","version":"1.10.9","mode":"auto","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"搜索...","empty":"找不到您查询的内容: ${query}","hits":"找到 ${hits} 条结果","hits_time":"找到 ${hits} 条结果（用时 ${time} 毫秒）"},"anonymous_image":"https://cdn.yunyoujun.cn/img/avatar/none.jpg","say":{"api":"https://el-bot-api.vercel.app/api/words/young"},"fireworks":{"colors":null},"vendors":{"host":"https://fastly.jsdelivr.net/npm/","darken":"https://fastly.jsdelivr.net/npm/darken@1.5.0"}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/hexo-theme-yun.js" type="module"></script><link rel="alternate" href="/atom.xml" title="愚人而已" type="application/atom+xml"><meta name="description" content="施工中 内容参考：  《深度学习入门：基于 Python 的理论与实现》(斋藤康毅) 上述书籍作者提供的代码  参数的更新实际问题中参数空间可能非常复杂，很难快速找到最优解。 SGDSGD 的策略很好理解，就是沿着梯度方向更新参数，寻找使损失函数值尽可能小的参数。可以写成如下式子： $$ \boldsymbol{W} \leftarrow \boldsymbol{W} - \eta \frac{\">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习入门：与学习相关的技巧">
<meta property="og:url" content="https://afool.top/learning/deep-learning-from-scratch-some-tricks-related-with-learning/">
<meta property="og:site_name" content="愚人而已">
<meta property="og:description" content="施工中 内容参考：  《深度学习入门：基于 Python 的理论与实现》(斋藤康毅) 上述书籍作者提供的代码  参数的更新实际问题中参数空间可能非常复杂，很难快速找到最优解。 SGDSGD 的策略很好理解，就是沿着梯度方向更新参数，寻找使损失函数值尽可能小的参数。可以写成如下式子： $$ \boldsymbol{W} \leftarrow \boldsymbol{W} - \eta \frac{\">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2022/11/05/1cVihLHDPk8z3K2.png">
<meta property="og:image" content="https://s2.loli.net/2022/11/05/Cdzb3iq8xgp4m9M.png">
<meta property="og:image" content="https://s2.loli.net/2022/11/05/7NsIKoBmYT4R6x2.png">
<meta property="og:image" content="https://s2.loli.net/2022/11/05/AT4OMalyiK5mEJ3.png">
<meta property="og:image" content="https://s2.loli.net/2022/11/05/4dN1YzWqM8vLkyn.png">
<meta property="og:image" content="https://s2.loli.net/2022/11/10/d9TXaiqe3nrFLGv.png">
<meta property="og:image" content="https://s2.loli.net/2022/11/10/wzlvkH3OnQyoBGJ.png">
<meta property="og:image" content="https://s2.loli.net/2022/11/10/vRxCPo5d1pUENQm.png">
<meta property="og:image" content="https://s2.loli.net/2022/11/10/4HlIvRQusAUzOqk.png">
<meta property="article:published_time" content="2022-11-04T15:36:13.000Z">
<meta property="article:modified_time" content="2022-11-10T14:06:53.190Z">
<meta property="article:author" content="零歌">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2022/11/05/1cVihLHDPk8z3K2.png"><script>(function() {
  if (CONFIG.mode !== 'auto') return
  const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches
  const setting = localStorage.getItem('darken-mode') || 'auto'
  if (setting === 'dark' || (prefersDark && setting !== 'light'))
    document.documentElement.classList.toggle('dark', true)
})()</script></head><body><script src="https://code.iconify.design/2/2.1.1/iconify.min.js"></script><script>// Define global variable
IconifyProviders = {
  // Empty prefix: overwrite default API provider configuration
  '': {
    // Use custom API first, use Iconify public API as backup
    resources: [
        'https://api.iconify.design',
    ],
    // Wait for 1 second before switching API hosts
    rotate: 1000,
  },
};</script><script defer src="https://fastly.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js" type="module"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js" type="module"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="文章目录"><span class="icon iconify" data-icon="ri:list-ordered"></span></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><span class="icon iconify" data-icon="ri:passport-line"></span></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="零歌"><img width="96" loading="lazy" src="/images/nekosense.jpeg" alt="零歌"></a><div class="site-author-name"><a href="/about/">零歌</a></div><span class="site-name">愚人而已</span><sub class="site-subtitle">以愚者之名攀上顶峰</sub><div class="site-description"></div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="首页"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:home-4-line"></span></span></a><div class="site-state-item"><a href="/archives/" title="归档"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:archive-line"></span></span><span class="site-state-item-count">46</span></a></div><div class="site-state-item"><a href="/categories/" title="分类"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:folder-2-line"></span></span><span class="site-state-item-count">5</span></a></div><div class="site-state-item"><a href="/tags/" title="标签"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="site-state-item-count">34</span></a></div><a class="site-state-item hty-icon-button" target="_blank" rel="noopener" href="https://yun.yunyoujun.cn" title="文档"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:settings-line"></span></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="/atom.xml" title="RSS" target="_blank" style="color:orange"><span class="icon iconify" data-icon="ri:rss-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/charliedu2000" title="GitHub" target="_blank" style="color:#6e5494"><span class="icon iconify" data-icon="ri:github-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://music.163.com/#/user/home?id=610609902" title="网易云音乐" target="_blank" style="color:#C20C0C"><span class="icon iconify" data-icon="ri:netease-cloud-music-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://space.bilibili.com/22660607" title="哔哩哔哩" target="_blank" style="color:#FF8EB3"><span class="icon iconify" data-icon="ri:bilibili-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="mailto:charliedu2000@hotmail.com" title="E-Mail" target="_blank" style="color:#8E71C1"><span class="icon iconify" data-icon="ri:mail-line"></span></a></div><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="旁友们" style="color:dodgerblue"><span class="icon iconify" data-icon="ri:genderless-line"></span></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><span class="icon iconify" data-icon="ri:contrast-2-line"></span></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E7%9A%84%E6%9B%B4%E6%96%B0"><span class="toc-number">1.</span> <span class="toc-text">参数的更新</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#SGD"><span class="toc-number">1.1.</span> <span class="toc-text">SGD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Momentum"><span class="toc-number">1.2.</span> <span class="toc-text">Momentum</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#AdaGrad"><span class="toc-number">1.3.</span> <span class="toc-text">AdaGrad</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9D%83%E9%87%8D%E7%9A%84%E5%88%9D%E5%A7%8B%E5%80%BC"><span class="toc-number">2.</span> <span class="toc-text">权重的初始值</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%80%BC%E5%8F%AF%E4%BB%A5%E4%B8%BA0%E5%90%97"><span class="toc-number">2.1.</span> <span class="toc-text">初始值可以为0吗</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%90%E8%97%8F%E5%B1%82%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%B1%82%E7%9A%84%E5%88%86%E5%B8%83"><span class="toc-number">2.2.</span> <span class="toc-text">隐藏层的激活函数层的分布</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ReLU-%E7%9A%84%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%80%BC"><span class="toc-number">2.3.</span> <span class="toc-text">ReLU 的权重初始值</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Batch-Normalization"><span class="toc-number">3.</span> <span class="toc-text">Batch Normalization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Batch-Norm-%E7%AE%97%E6%B3%95%EF%BC%88%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%EF%BC%89"><span class="toc-number">3.1.</span> <span class="toc-text">Batch Norm 算法（前向传播）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">4.</span> <span class="toc-text">正则化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">4.1.</span> <span class="toc-text">过拟合</span></a></li></ol></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article" style="--smc-primary:#0078E7;"><link itemprop="mainEntityOfPage" href="https://afool.top/learning/deep-learning-from-scratch-some-tricks-related-with-learning/"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="零歌"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="愚人而已"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">深度学习入门：与学习相关的技巧</h1><div class="post-meta"><div class="post-time" style="display:block"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-line"></span></span> <time title="创建时间：2022-11-04 23:36:13" itemprop="dateCreated datePublished" datetime="2022-11-04T23:36:13+08:00">2022-11-04</time></div><span class="post-count"><span class="post-symbolcount"><span class="post-meta-item-icon" title="本文字数"><span class="icon iconify" data-icon="ri:file-word-line"></span></span> <span title="本文字数">2.2k</span><span class="post-meta-divider">-</span><span class="post-meta-item-icon" title="阅读时长"><span class="icon iconify" data-icon="ri:timer-line"></span></span> <span title="阅读时长">8m</span></span></span><div class="post-classify"><span class="post-category"> <span class="post-meta-item-icon" style="margin-right:3px;"><span class="icon iconify" data-icon="ri:folder-line"></span></span><span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category-item" href="/categories/%E5%BF%AB%E5%8E%BB%E5%AD%A6%E4%B9%A0/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">快去学习</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag-item" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="tag-name">深度学习</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body"><p><strong>施工中</strong></p>
<p>内容参考：</p>
<ul>
<li>《深度学习入门：基于 Python 的理论与实现》(斋藤康毅)</li>
<li><a target="_blank" rel="noopener" href="https://github.com/oreilly-japan/deep-learning-from-scratch">上述书籍作者提供的代码</a></li>
</ul>
<h2 id="参数的更新"><a href="#参数的更新" class="headerlink" title="参数的更新"></a>参数的更新</h2><p>实际问题中参数空间可能非常复杂，很难快速找到最优解。</p>
<h3 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h3><p>SGD 的策略很好理解，就是沿着梯度方向更新参数，寻找使损失函数值尽可能小的参数。可以写成如下式子：</p>
<p>$$ \boldsymbol{W} \leftarrow \boldsymbol{W} - \eta \frac{\partial L}{\partial \boldsymbol{W}} $$</p>
<p>其中 $\boldsymbol{W}$ 表示要更新的权重参数，$\frac{\partial L}{\partial \boldsymbol{W}}$ 是损失函数关于 $\boldsymbol{W}$ 的梯度，$\eta$ 表示学习率（一般取0.1或0.01等事先确定的值）。</p>
<p>将 SGD 的方法用类的形式实现：</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">SGD</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>lr <span class="token operator">=</span> lr  <span class="token comment"># 学习率</span>

    <span class="token keyword">def</span> <span class="token function">update</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> params<span class="token punctuation">,</span> grads<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> key <span class="token keyword">in</span> params<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            params<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">-=</span> self<span class="token punctuation">.</span>lr <span class="token operator">*</span> grads<span class="token punctuation">[</span>key<span class="token punctuation">]</span></code></pre>

<p><code>SGD</code> 可以作为代码中的 <code>optimizer</code> 来进行参数更新。只要不同的最优化方法都实现 <code>update(params, grads)</code>，就可以用 <code>optimizer.update(params, grads)</code> 的形式在代码中完成更新参数的步骤。</p>
<p>从上面可以看出，SGD 的实现也很简单。然而在某些情况下（比如梯度并没有指向最小值的方向），SGD 的效率并不理想。例如，对于函数 $f(x, y) &#x3D; \frac{1}{20} x^2 + y^2$，它的函数图形和等高线如图所示：</p>
<p><img src="https://s2.loli.net/2022/11/05/1cVihLHDPk8z3K2.png" alt="f(x, y) 的图像和等高线" loading="lazy"></p>
<p>梯度如图所示：</p>
<p><img src="https://s2.loli.net/2022/11/05/Cdzb3iq8xgp4m9M.png" alt="f(x, y) 的梯度" loading="lazy"></p>
<p>显然函数在 $(x, y) &#x3D; (0, 0)$ 处取得最小值，但是在很多地方，梯度并不指向 $(0, 0)$。对这个函数使用 SGD，比方说从 $(x, y) &#x3D; (-7.0, -2.0)$ 处开始搜索，更新路径会像这样：</p>
<p><img src="https://s2.loli.net/2022/11/05/7NsIKoBmYT4R6x2.png" alt="更新路径" loading="lazy"></p>
<p>由于很多地方的梯度不指向 $(0, 0)$，这个更新路径呈“之”字形，显然不是很理想，效率不高。所以我们需要其他的更新方法。</p>
<h3 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h3><p>Momentum 方法引入了“速度”，“速度”的更新可以看作物体在梯度方向上受力：</p>
<p>$$ \boldsymbol{v} \leftarrow \alpha \boldsymbol{v} - \eta \frac{\partial L}{\partial \boldsymbol{W}} $$</p>
<p>$$ \boldsymbol{W} \leftarrow \boldsymbol{W} + \boldsymbol{v} $$</p>
<p>$\alpha \boldsymbol{v}$ 在物体不受力时会使速度衰减（$\alpha$ 一般取零点几的值），可以看作空气等带来的阻力。</p>
<p>Momentum 方法的代码实现：</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Momentum</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>lr <span class="token operator">=</span> lr
        self<span class="token punctuation">.</span>momentum <span class="token operator">=</span> momentum
        self<span class="token punctuation">.</span>v <span class="token operator">=</span> <span class="token boolean">None</span>

    <span class="token keyword">def</span> <span class="token function">update</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> params<span class="token punctuation">,</span> grads<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>v <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>v <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>
            <span class="token keyword">for</span> key<span class="token punctuation">,</span> val <span class="token keyword">in</span> params<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                self<span class="token punctuation">.</span>v<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>val<span class="token punctuation">)</span>  <span class="token comment"># “速度”的形状与参数相同</span>

        <span class="token keyword">for</span> key <span class="token keyword">in</span> params<span class="token punctuation">.</span>keys<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>v<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>momentum <span class="token operator">*</span> self<span class="token punctuation">.</span>v<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>lr <span class="token operator">*</span> grads<span class="token punctuation">[</span>key<span class="token punctuation">]</span>
            params<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">+=</span> self<span class="token punctuation">.</span>v<span class="token punctuation">[</span>key<span class="token punctuation">]</span></code></pre>

<p>对于之前的问题，Momentum 方法的更新路径：</p>
<p><img src="https://s2.loli.net/2022/11/05/AT4OMalyiK5mEJ3.png" alt="Momentum 方法的更新路径" loading="lazy"></p>
<h3 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h3><p>AdaGrad 方法基于“学习率衰减”的想法：随着学习的进行，使学习率逐渐减小。实际上，AdaGrad 方法在学习过程中会为参数的每个元素调整学习率。</p>
<p>$$ \boldsymbol{h} \leftarrow \boldsymbol{h} + \frac{\partial L}{\partial \boldsymbol{W}} \odot \frac{\partial L}{\partial \boldsymbol{W}} $$</p>
<p>$$ \boldsymbol{W} \leftarrow \boldsymbol{W} - \eta \frac{1}{\sqrt{\boldsymbol{h}}} \frac{\partial L}{\partial \boldsymbol{W}} $$</p>
<p>$\odot$ 表示对应矩阵元素的乘法，$\boldsymbol{h}$ 表示以前所有梯度值的平方和。显然参数中变动较大的元素的学习率会变得更小（准确地说是学习率会被乘以一个更小的因数）。</p>
<p>实现过程：</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">AdaGrad</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>lr <span class="token operator">=</span> lr
        self<span class="token punctuation">.</span>h <span class="token operator">=</span> <span class="token boolean">None</span>

    <span class="token keyword">def</span> <span class="token function">update</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> params<span class="token punctuation">,</span> grads<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>h <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>h <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>
            <span class="token keyword">for</span> key<span class="token punctuation">,</span> val <span class="token keyword">in</span> params<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                self<span class="token punctuation">.</span>h<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>val<span class="token punctuation">)</span>

        <span class="token keyword">for</span> key <span class="token keyword">in</span> params<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>h<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">+=</span> grads<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">*</span> grads<span class="token punctuation">[</span>key<span class="token punctuation">]</span>
            params<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">-=</span> self<span class="token punctuation">.</span>lr <span class="token operator">*</span> grads<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">/</span> <span class="token punctuation">(</span>np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>self<span class="token punctuation">.</span>h<span class="token punctuation">[</span>key<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1e-7</span><span class="token punctuation">)</span>  <span class="token comment"># 1e-7避免将0用作除数</span></code></pre>

<p>这个方法在 $f(x, y)$ 中的更新路径：</p>
<p><img src="https://s2.loli.net/2022/11/05/4dN1YzWqM8vLkyn.png" alt="AdaGrad 方法的更新路径" loading="lazy"></p>
<p>因为 <code>y</code> 方向的更新幅度较大，在这个方向上的学习率衰减也就更明显。</p>
<p>当然还有别的方法，比如融合了 Momentum 和 AdaGrad 的 Adam 方法，就不再说明了。<del>（因为原书中也只给了相关论文和代码实现。）</del></p>
<p>使用不同的方法可以得到不同的更新路径，对于前面的问题来说 AdaGrad 能得到比较理想的路径，但如果换一个问题，可能其他方法会有更好的结果。<del>（那是自然。）</del></p>
<h2 id="权重的初始值"><a href="#权重的初始值" class="headerlink" title="权重的初始值"></a>权重的初始值</h2><h3 id="初始值可以为0吗"><a href="#初始值可以为0吗" class="headerlink" title="初始值可以为0吗"></a>初始值可以为0吗</h3><p><strong>权值衰减</strong>是一种抑制过拟合的技巧，它以减小权重参数的值为目的进行学习。如果想减小权重，最好的方法是一开始就把权重设定为较小的值。那把权重的初始值设为0（说到更普遍的情况，其实是把权重初始值设置为一样的值）呢？</p>
<p>考虑误差反向传播就可以发现，将权重初始值设置为一样的值，反向传播时也会被更新为一样的值。都是一样的值的话神经网络为什么要有许多权重呢？</p>
<p><del>原来如此，所以必须随机生成初始值！</del></p>
<h3 id="隐藏层的激活函数层的分布"><a href="#隐藏层的激活函数层的分布" class="headerlink" title="隐藏层的激活函数层的分布"></a>隐藏层的激活函数层的分布</h3><p><del>这里书上举的例子就不那么详细地复述了……</del></p>
<p>向一个5层神经网络传入随机生成的输入数据，激活函数使用 sigmoid 函数，权重使用标准差为1的高斯分布随机生成，用直方图绘制出各层激活值（激活函数的输出）的分布：</p>
<p><img src="https://s2.loli.net/2022/11/10/d9TXaiqe3nrFLGv.png" alt="使用标准差为1的高斯分布作为权重初始值时的各层激活值的分布" loading="lazy"></p>
<p>可以发现，各层的激活值都集中在0和1附近。对于 sigmoid 函数，在0和1附近的导数都比较接近0，那么这样的数据会造成反向传播中梯度的值不断变小，也就是所谓“<strong>梯度消失</strong>”</p>
<p>如果将权重的标准差设为0.01，各层的激活值分布就会变成：</p>
<p><img src="https://s2.loli.net/2022/11/10/wzlvkH3OnQyoBGJ.png" alt="使用标准差为1的高斯分布作为权重初始值时的各层激活值的分布" loading="lazy"></p>
<p>激活值集中在0.5附近，不会有梯度消失的问题，但是分布太过集中。显然不能让几乎所有的神经元都输出一样的值，实际应用中一般使用“Xavier 初始值”，可以避免这些问题。</p>
<p>Xavier 初始值控制权重尺度（标准差）的方式是：如果前一层的节点数为n，则初始值使用标准差为 $\frac{1}{\sqrt{n}}$ 的分布。</p>
<p>实现方式可以是：</p>
<pre class="language-python" data-language="python"><code class="language-python">pre_node_num <span class="token operator">=</span> <span class="token number">100</span> <span class="token comment"># 前一层的节点数</span>
w <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>pre_node_num<span class="token punctuation">,</span> node_num<span class="token punctuation">)</span> <span class="token operator">/</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>pre_node_num<span class="token punctuation">)</span></code></pre>

<p>使用 Xavier 初始值得到的各层激活值分布：</p>
<p><img src="https://s2.loli.net/2022/11/10/vRxCPo5d1pUENQm.png" alt="使用 Xavier 初始值作为权重初始值时的各层激活值的分布" loading="lazy"></p>
<p>比之前的分布更有广度。</p>
<blockquote>
<p>如果用 tanh 函数（双曲线函数）代替 sigmoid 函数，可以改善后面的层激活值歪斜的问题。</p>
</blockquote>
<h3 id="ReLU-的权重初始值"><a href="#ReLU-的权重初始值" class="headerlink" title="ReLU 的权重初始值"></a>ReLU 的权重初始值</h3><blockquote>
<p>Xavier 初始值是以激活函数是线性函数为前提而推导出来的。因为 sigmoid 函数和 tanh 函数左右对称，且中央附近可以视作线性函数，所以适合使用 Xavier 初始值。但当激活函数使用 ReLU 时，一般推荐使用 ReLU 专用的初始值，也就是 Kaiming He 等人推荐的初始值，也称为“He 初始值”。当前一层的节点数为 n 时，He 初始值使用标准差为 $\sqrt{\frac{2}{n}}$ 的高斯分布。当 Xavier 初始值是 $\sqrt{\frac{1}{n}}$ 时，（直观上）可以解释为，因为 ReLU 的负值区域的值为0，为了使它更有广度，所以需要2倍的系数。</p>
</blockquote>
<p>用作者给出的 <code>ch06/weight_init_compare.py</code> 可以比较使用不同权重初始值时神经网络的学习情况。</p>
<h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><p>前面调整权重的初始值是为了使各层的激活值分布有合适的广度，还有一种办法可以“强制性”地调整激活值的分布来达到这个目的。</p>
<h3 id="Batch-Norm-算法（前向传播）"><a href="#Batch-Norm-算法（前向传播）" class="headerlink" title="Batch Norm 算法（前向传播）"></a>Batch Norm 算法（前向传播）</h3><blockquote>
<p>Batch Norm有以下优点。</p>
<ul>
<li>可以使学习快速进行（可以增大学习率）。</li>
<li>不那么依赖初始值（对于初始值不用那么神经质）。</li>
<li>抑制过拟合（降低Dropout等的必要性）。</li>
</ul>
</blockquote>
<p>Batch Norm 的思路是调整各层的激活值分布使其拥有适当的广度，为此需要在神经网络中插入对数据分布进行正规化的层（Batch Norm层）：</p>
<p><img src="https://s2.loli.net/2022/11/10/4HlIvRQusAUzOqk.png" alt="使用了 Batch Normalization 的神经网络的例子" loading="lazy"></p>
<p>Batch Norm 是以学习时的 mini-batch 为单位，按 mini-batch 进行正规化，数学式表示：</p>
<p>$$ \mu_B \leftarrow \frac{1}{m} \sum_{i &#x3D; 1}^m x_i $$</p>
<p>$$ \sigma_B^2 \leftarrow \frac{1}{m} \sum_{i &#x3D; 1}^m (x_i - \mu_B)^2 $$</p>
<p>$$ \hat{x}_i \leftarrow \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}} $$</p>
<p>对 mini-batch 的 m 个输入数据的集合 $B &#x3D; { x_1, x_2, … , x_m }$ 求均值和方差 。然后，对输入数据进行均值为0、方差为1（合适的分布）的正规化。其中 $\epsilon$ 是一个微小值。</p>
<p>Batch Norm 层会将正规化后的数据进行缩放和平移的变换：</p>
<p>$$ y_i \leftarrow \gamma \hat{x}_i + \beta $$</p>
<p>一开始参数 $\gamma &#x3D; 1$，$\beta &#x3D; 0$，可以通过学习调整到合适的值。</p>
<h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><h3 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h3></div></section><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>零歌</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="https://afool.top/learning/deep-learning-from-scratch-some-tricks-related-with-learning/" title="深度学习入门：与学习相关的技巧">https://afool.top/learning/deep-learning-from-scratch-some-tricks-related-with-learning/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><span class="icon iconify" data-icon="ri:creative-commons-line"></span><span class="icon iconify" data-icon="ri:creative-commons-by-line"></span><span class="icon iconify" data-icon="ri:creative-commons-nc-line"></span><span class="icon iconify" data-icon="ri:creative-commons-sa-line"></span></a> 许可协议。</li></ul></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/learning/deep-learning-from-scratch-cnn/" rel="prev" title="深度学习入门：卷积神经网络"><span class="icon iconify" data-icon="ri:arrow-left-s-line"></span><span class="post-nav-text">深度学习入门：卷积神经网络</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/learning/deep-learning-from-scratch-error-back-propagation/" rel="next" title="深度学习入门：误差反向传播法"><span class="post-nav-text">深度学习入门：误差反向传播法</span><span class="icon iconify" data-icon="ri:arrow-right-s-line"></span></a></div></div></div><div class="hty-card" id="comment"><div class="comment-tooltip text-center"><span>留个爪印吧~</span><br></div><div id="tcomment"></div><script type="module">import { getScript } from '/js/utils.js'

getScript("https://fastly.jsdelivr.net/npm/twikoo@latest/dist/twikoo.all.min.js", () => {
  const twikooConfig = {"enable":true,"envId":"https://twikoo-afool.vercel.app/"}
  twikooConfig.el = '#tcomment'
  twikoo.init(twikooConfig)
}, window.twikoo);</script></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2020 – 2022 </span><span class="with-love" id="animate"><span class="icon iconify" data-icon="ri:cloud-line"></span></span><span class="author"> 零歌</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v6.3.0</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.10.9</span></div></footer></div><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><span class="icon iconify" data-icon="ri:arrow-up-s-line"></span><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a></body></html>