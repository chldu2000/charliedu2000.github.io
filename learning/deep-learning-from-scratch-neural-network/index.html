<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="零歌"><meta name="copyright" content="零歌"><meta name="generator" content="Hexo 6.3.0"><meta name="theme" content="hexo-theme-yun"><title>深度学习入门：神经网络 | 愚人而已</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/star-markdown-css@0.4.1/dist/yun/yun-markdown.min.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/prism-theme-vars/base.css"><script src="https://fastly.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>function initScrollReveal() {
  [".post-card",".markdown-body img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
}
document.addEventListener("DOMContentLoaded", initScrollReveal);
document.addEventListener("pjax:success", initScrollReveal);
</script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://fastly.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"></script><script type="module">import { renderKatex } from '/js/utils.js'
document.addEventListener("DOMContentLoaded", () => {
  renderKatex({
    ...{},
    ...true?.options,
  });
});</script><link rel="icon" type="image/svg+xml" href="/afool.svg"><link rel="mask-icon" href="/afool.svg" color="#0078E7"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="preconnect" href="https://fastly.jsdelivr.net/npm/" crossorigin><script id="yun-config">
    window.Yun = {}
    window.CONFIG = {"hostname":"afool.top","root":"/","title":"愚人而已","version":"1.10.9","mode":"auto","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"搜索...","empty":"找不到您查询的内容: ${query}","hits":"找到 ${hits} 条结果","hits_time":"找到 ${hits} 条结果（用时 ${time} 毫秒）"},"anonymous_image":"https://cdn.yunyoujun.cn/img/avatar/none.jpg","say":{"api":"https://el-bot-api.vercel.app/api/words/young"},"fireworks":{"colors":null},"vendors":{"host":"https://fastly.jsdelivr.net/npm/","darken":"https://fastly.jsdelivr.net/npm/darken@1.5.0"}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/hexo-theme-yun.js" type="module"></script><link rel="alternate" href="/atom.xml" title="愚人而已" type="application/atom+xml"><meta name="description" content="内容参考：  《深度学习入门：基于 Python 的理论与实现》(斋藤康毅) 上述书籍作者提供的代码  代码中如果出现 np，应该都是因为忽略了 import numpy as np，请不要在意。 从感知机到神经网络通过前面学的内容可以看出来感知机也能表示复杂的内容，但显然确定权重是个麻烦事。好消息是，现在有神经网络这个东西能自动从数据中学习到合适的权重参数。那就看看吧。 用图表示神经网络：  中">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习入门：神经网络">
<meta property="og:url" content="https://afool.top/learning/deep-learning-from-scratch-neural-network/">
<meta property="og:site_name" content="愚人而已">
<meta property="og:description" content="内容参考：  《深度学习入门：基于 Python 的理论与实现》(斋藤康毅) 上述书籍作者提供的代码  代码中如果出现 np，应该都是因为忽略了 import numpy as np，请不要在意。 从感知机到神经网络通过前面学的内容可以看出来感知机也能表示复杂的内容，但显然确定权重是个麻烦事。好消息是，现在有神经网络这个东西能自动从数据中学习到合适的权重参数。那就看看吧。 用图表示神经网络：  中">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2022/10/06/56M1dzptNKeiXIq.png">
<meta property="og:image" content="https://s2.loli.net/2022/10/06/9bpEaniNYfl7eq3.png">
<meta property="og:image" content="https://s2.loli.net/2022/10/06/sFzIqrO1hoN926V.png">
<meta property="og:image" content="https://s2.loli.net/2022/10/06/fhCzFJSnbaIRpym.png">
<meta property="og:image" content="https://s2.loli.net/2022/10/06/hNPZtR67nGjQyBq.png">
<meta property="og:image" content="https://s2.loli.net/2022/10/06/GSolM6v7B3AcWDd.png">
<meta property="og:image" content="https://s2.loli.net/2022/10/06/vbICLGd341DRlnz.png">
<meta property="og:image" content="https://s2.loli.net/2022/10/06/p8sriFWC2tVNb1O.png">
<meta property="og:image" content="https://s2.loli.net/2022/10/06/GkWuzmDV1QCqM2E.png">
<meta property="og:image" content="https://s2.loli.net/2022/10/06/T3uDapWMw1eK9Xq.png">
<meta property="og:image" content="https://s2.loli.net/2022/10/06/Q8c1eOVod6WFSZu.png">
<meta property="og:image" content="https://s2.loli.net/2022/10/06/hANMxfLWD6pBvl5.png">
<meta property="og:image" content="https://s2.loli.net/2022/10/07/l25NM8BjZ3ohcfg.png">
<meta property="og:image" content="https://s2.loli.net/2022/10/07/3eWDaKFkYHp6riU.png">
<meta property="article:published_time" content="2022-10-05T13:17:12.000Z">
<meta property="article:modified_time" content="2022-11-07T08:43:40.831Z">
<meta property="article:author" content="零歌">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="神经网络">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2022/10/06/56M1dzptNKeiXIq.png"><script>(function() {
  if (CONFIG.mode !== 'auto') return
  const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches
  const setting = localStorage.getItem('darken-mode') || 'auto'
  if (setting === 'dark' || (prefersDark && setting !== 'light'))
    document.documentElement.classList.toggle('dark', true)
})()</script></head><body><script src="https://code.iconify.design/2/2.1.1/iconify.min.js"></script><script>// Define global variable
IconifyProviders = {
  // Empty prefix: overwrite default API provider configuration
  '': {
    // Use custom API first, use Iconify public API as backup
    resources: [
        'https://api.iconify.design',
    ],
    // Wait for 1 second before switching API hosts
    rotate: 1000,
  },
};</script><script defer src="https://fastly.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js" type="module"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js" type="module"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="文章目录"><span class="icon iconify" data-icon="ri:list-ordered"></span></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><span class="icon iconify" data-icon="ri:passport-line"></span></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="零歌"><img width="96" loading="lazy" src="/images/nekosense.jpeg" alt="零歌"></a><div class="site-author-name"><a href="/about/">零歌</a></div><span class="site-name">愚人而已</span><sub class="site-subtitle">以愚者之名攀上顶峰</sub><div class="site-description"></div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="首页"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:home-4-line"></span></span></a><div class="site-state-item"><a href="/archives/" title="归档"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:archive-line"></span></span><span class="site-state-item-count">46</span></a></div><div class="site-state-item"><a href="/categories/" title="分类"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:folder-2-line"></span></span><span class="site-state-item-count">5</span></a></div><div class="site-state-item"><a href="/tags/" title="标签"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="site-state-item-count">34</span></a></div><a class="site-state-item hty-icon-button" target="_blank" rel="noopener" href="https://yun.yunyoujun.cn" title="文档"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:settings-line"></span></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="/atom.xml" title="RSS" target="_blank" style="color:orange"><span class="icon iconify" data-icon="ri:rss-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/charliedu2000" title="GitHub" target="_blank" style="color:#6e5494"><span class="icon iconify" data-icon="ri:github-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://music.163.com/#/user/home?id=610609902" title="网易云音乐" target="_blank" style="color:#C20C0C"><span class="icon iconify" data-icon="ri:netease-cloud-music-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://space.bilibili.com/22660607" title="哔哩哔哩" target="_blank" style="color:#FF8EB3"><span class="icon iconify" data-icon="ri:bilibili-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="mailto:charliedu2000@hotmail.com" title="E-Mail" target="_blank" style="color:#8E71C1"><span class="icon iconify" data-icon="ri:mail-line"></span></a></div><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="旁友们" style="color:dodgerblue"><span class="icon iconify" data-icon="ri:genderless-line"></span></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><span class="icon iconify" data-icon="ri:contrast-2-line"></span></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8E%E6%84%9F%E7%9F%A5%E6%9C%BA%E5%88%B0%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.</span> <span class="toc-text">从感知机到神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BD%AE%E5%88%B0%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E4%B8%8A%E5%9C%BA%E2%80%A6%E2%80%A6"><span class="toc-number">1.1.</span> <span class="toc-text">轮到激活函数上场……</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">1.1.1.</span> <span class="toc-text">常用的激活函数</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E7%BB%B4%E6%95%B0%E7%BB%84%E7%9A%84%E8%BF%90%E7%AE%97"><span class="toc-number">2.</span> <span class="toc-text">多维数组的运算</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%86%85%E7%A7%AF"><span class="toc-number">2.1.</span> <span class="toc-text">神经网络的内积</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.</span> <span class="toc-text">神经网络的实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%A1%E5%8F%B7%E4%BC%A0%E9%80%92"><span class="toc-number">3.1.</span> <span class="toc-text">信号传递</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BE%93%E5%87%BA%E5%B1%82%E7%9A%84%E8%AE%BE%E8%AE%A1"><span class="toc-number">4.</span> <span class="toc-text">输出层的设计</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#softmax-%E5%87%BD%E6%95%B0"><span class="toc-number">4.1.</span> <span class="toc-text">softmax 函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BE%93%E5%87%BA%E5%B1%82%E7%9A%84%E7%A5%9E%E7%BB%8F%E5%85%83%E6%95%B0%E9%87%8F"><span class="toc-number">4.2.</span> <span class="toc-text">输出层的神经元数量</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%99%85%E9%97%AE%E9%A2%98%EF%BC%9A%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB"><span class="toc-number">5.</span> <span class="toc-text">实际问题：手写数字识别</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%B9%E5%A4%84%E7%90%86"><span class="toc-number">5.1.</span> <span class="toc-text">批处理</span></a></li></ol></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article" style="--smc-primary:#0078E7;"><link itemprop="mainEntityOfPage" href="https://afool.top/learning/deep-learning-from-scratch-neural-network/"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="零歌"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="愚人而已"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">深度学习入门：神经网络</h1><div class="post-meta"><div class="post-time" style="display:block"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-line"></span></span> <time title="创建时间：2022-10-05 21:17:12" itemprop="dateCreated datePublished" datetime="2022-10-05T21:17:12+08:00">2022-10-05</time></div><span class="post-count"><span class="post-symbolcount"><span class="post-meta-item-icon" title="本文字数"><span class="icon iconify" data-icon="ri:file-word-line"></span></span> <span title="本文字数">4.3k</span><span class="post-meta-divider">-</span><span class="post-meta-item-icon" title="阅读时长"><span class="icon iconify" data-icon="ri:timer-line"></span></span> <span title="阅读时长">18m</span></span></span><div class="post-classify"><span class="post-category"> <span class="post-meta-item-icon" style="margin-right:3px;"><span class="icon iconify" data-icon="ri:folder-line"></span></span><span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category-item" href="/categories/%E5%BF%AB%E5%8E%BB%E5%AD%A6%E4%B9%A0/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">快去学习</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag-item" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="tag-name">深度学习</span></a><a class="tag-item" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="tag-name">神经网络</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body"><p>内容参考：</p>
<ul>
<li>《深度学习入门：基于 Python 的理论与实现》(斋藤康毅)</li>
<li><a target="_blank" rel="noopener" href="https://github.com/oreilly-japan/deep-learning-from-scratch">上述书籍作者提供的代码</a></li>
</ul>
<p>代码中如果出现 <code>np</code>，应该都是因为忽略了 <code>import numpy as np</code>，请不要在意。</p>
<h2 id="从感知机到神经网络"><a href="#从感知机到神经网络" class="headerlink" title="从感知机到神经网络"></a>从感知机到神经网络</h2><p>通过前面学的内容可以看出来感知机也能表示复杂的内容，但显然确定权重是个麻烦事。好消息是，现在有神经网络这个东西能自动从数据中学习到合适的权重参数。那就看看吧。</p>
<p>用图表示神经网络：</p>
<p><img src="https://s2.loli.net/2022/10/06/56M1dzptNKeiXIq.png" alt="神经网络的基本结构" loading="lazy"></p>
<p>中间层有时也被称为<strong>隐藏层</strong>（“肉眼看不见”）。</p>
<p>从图中可以看出，神经网络的形状和感知机很像。<em>实际上，就神经元的连接方式而言，与感知机并没有任何差异</em>。既然如此，它们有什么区别呢？就从信号的传递方式入手吧。</p>
<p>在探究神经网络之前，需要再看一下感知机，这是感知机的网络结构（这个图并没有表示出偏置，要体现偏置的话需要再加一个表示偏置的输入信号）：</p>
<p><img src="https://s2.loli.net/2022/10/06/9bpEaniNYfl7eq3.png" alt="感知机的结构" loading="lazy"></p>
<p>用数学式表示这个感知机：</p>
<p>$$ y &#x3D; \begin{cases} 0 &amp; (b + w_1 x_1 + w_2 x_2 \le 0) \\ 1 &amp; (b + w_1 x_1 + w_2 x_2 \gt 0) \end{cases} $$</p>
<p>这个数学式可以改写成：</p>
<p>$$ y &#x3D; h(b + w_1 x_1 + w_2 x_2) $$</p>
<p>其中：</p>
<p>$$ h(x) &#x3D; \begin{cases} 0 &amp; (x \le 0) \\ 1 &amp; (x \gt 0) \end{cases} $$</p>
<p>输入信号的总和被函数 $h(x)$ 转换之后才是输出 $y$。</p>
<h3 id="轮到激活函数上场……"><a href="#轮到激活函数上场……" class="headerlink" title="轮到激活函数上场……"></a>轮到激活函数上场……</h3><p>上面的 $h(x)$ 通常被称为<strong>激活函数</strong>，它决定如何激活输入信号的总和。那么上面的式子也可以表示为：</p>
<p>$$ a &#x3D; b + w_1 x_1 + w_2 x_2 \\ y &#x3D; h(a) $$</p>
<p>也就是说，计算输出的过程可以分为：计算输入信号的总和、用激活函数转换这一总和。在神经元中明确表示出激活函数的计算过程（下图包含偏置）：</p>
<p><img src="https://s2.loli.net/2022/10/06/sFzIqrO1hoN926V.png" alt="明确表示出激活函数的计算过程" loading="lazy"></p>
<h4 id="常用的激活函数"><a href="#常用的激活函数" class="headerlink" title="常用的激活函数"></a>常用的激活函数</h4><ul>
<li>阶跃函数<ul>
<li>输入不超过0时输出0，超过0时输出1。</li>
<li><img src="https://s2.loli.net/2022/10/06/fhCzFJSnbaIRpym.png" alt="阶跃函数的图形" loading="lazy"></li>
</ul>
</li>
<li>sigmoid 函数<ul>
<li>$h(x) &#x3D; \frac{1}{1 + exp(-x)}$，其中 $exp(-x)$ 表示 $e^{-x}$</li>
<li><img src="https://s2.loli.net/2022/10/06/hNPZtR67nGjQyBq.png" alt="sigmoid 函数的图形" loading="lazy"></li>
</ul>
</li>
</ul>
<p>可行的实现：</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># 阶跃函数，支持 numpy 数组的实现</span>
<span class="token keyword">def</span> <span class="token function">step_function</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    y <span class="token operator">=</span> x <span class="token operator">></span> <span class="token number">0</span>  <span class="token comment"># 对 numpy 数组进行不等号运算后，数组中的各个元素都会进行相应运算，生成一个布尔型数组</span>
    <span class="token keyword">return</span> y<span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>int32<span class="token punctuation">)</span>  <span class="token comment"># 将布尔型数组转换为 int32</span>

<span class="token keyword">def</span> <span class="token function">sigmoid</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token number">1</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 由于 numpy 的广播功能，该实现可以支持 numpy 数组</span></code></pre>

<p>显然：</p>
<ul>
<li>sigmoid 函数图像是一条平滑曲线，阶跃函数以0为界，输出发生急剧变化；</li>
<li>阶跃函数只返回0或1，sigmoid 函数返回连续的实数值；</li>
<li>两者形状相似，且都是非线性函数；</li>
</ul>
<blockquote>
<p>激活函数必须使用非线性函数，为什么？</p>
<p>线性函数的问题在于，不管如何加深层数，总是存在与之等效的“无隐藏层的神经网络”。为了具体地（稍微直观地）理解这一点，我们来思考下面这个简单的例子。这里我们考虑把线性函数 $h(x) &#x3D; cx$ 作为激活函数，把 $y(x) &#x3D; h(h(h(x)))$ 的运算对应3层神经网络A。这个运算会进行 $y(x) &#x3D; c × c × c × x$ 的乘法运算，但是同样的处理可以由 $y(x) &#x3D; ax$（注意，$a &#x3D; c^3$）这一次乘法运算（即没有隐藏层的神经网络）来表示。如本例所示，使用线性函数时，无法发挥多层网络带来的优势。因此，为了发挥叠加层所带来的优势，激活函数必须使用非线性函数。</p>
</blockquote>
<p>据书上所说，最近使用 <strong>ReLU</strong> 函数的更多。这个函数很简单，输入大于0时直接输出该值，输入不大于0时输出0。</p>
<p>ReLU 函数的一种实现：</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">relu</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> np<span class="token punctuation">.</span>maximum<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> x<span class="token punctuation">)</span></code></pre>

<h2 id="多维数组的运算"><a href="#多维数组的运算" class="headerlink" title="多维数组的运算"></a>多维数组的运算</h2><p>可以直接用 <code>numpy.array()</code> 来生成多维数组，比如：</p>
<pre class="language-python" data-language="python"><code class="language-python">x <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre>

<p>可以用 <code>numpy.ndim()</code> 获取数组的维数，用数组实例的属性变量 <code>shape</code> （元组类型）获取其形状。</p>
<p>我们最常用的多维数组的运算应该是矩阵（二维数组）乘法了（我猜）。矩阵乘法怎么算就不说了，左边列数等于右边行数就行，在代码中可以用 <code>numpy.dot()</code> 来完成。矩阵乘法的例子：</p>
<pre class="language-python" data-language="python"><code class="language-python">A <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
B <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>A<span class="token punctuation">,</span> B<span class="token punctuation">)</span><span class="token punctuation">)</span>

A <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
B <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>A<span class="token punctuation">,</span> B<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre>

<p>输出：</p>
<pre class="language-bash" data-language="bash"><code class="language-bash"><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">19</span> <span class="token number">22</span><span class="token punctuation">]</span>
 <span class="token punctuation">[</span><span class="token number">43</span> <span class="token number">50</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
<span class="token punctuation">[</span><span class="token number">23</span> <span class="token number">53</span> <span class="token number">83</span><span class="token punctuation">]</span></code></pre>

<h3 id="神经网络的内积"><a href="#神经网络的内积" class="headerlink" title="神经网络的内积"></a>神经网络的内积</h3><p>矩阵乘法又和神经网络有什么关系呢？看这个网络：</p>
<p><img src="https://s2.loli.net/2022/10/06/GSolM6v7B3AcWDd.png" alt="神经网络和矩阵乘法" loading="lazy"></p>
<p>从图上可以看出，这个神经网络的计算相当于 X 矩阵点乘 W 矩阵，可以用矩阵运算一次性算出结果：</p>
<pre class="language-python" data-language="python"><code class="language-python">x <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># x1 x2</span>
w <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 权重</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>w<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>w<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> w<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre>

<p>输出结果：</p>
<pre class="language-bash" data-language="bash"><code class="language-bash"><span class="token punctuation">[</span><span class="token number">1</span> <span class="token number">2</span><span class="token punctuation">]</span>
<span class="token punctuation">(</span><span class="token number">2</span>,<span class="token punctuation">)</span>
<span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span> <span class="token number">3</span> <span class="token number">5</span><span class="token punctuation">]</span>
 <span class="token punctuation">[</span><span class="token number">2</span> <span class="token number">4</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
<span class="token punctuation">(</span><span class="token number">2</span>, <span class="token number">3</span><span class="token punctuation">)</span>
<span class="token punctuation">[</span> <span class="token number">5</span> <span class="token number">11</span> <span class="token number">17</span><span class="token punctuation">]</span></code></pre>

<p>所以我们就可以用矩阵运算简化信号传递的过程，可喜可贺。</p>
<h2 id="神经网络的实现"><a href="#神经网络的实现" class="headerlink" title="神经网络的实现"></a>神经网络的实现</h2><p><img src="https://s2.loli.net/2022/10/06/vbICLGd341DRlnz.png" alt="要实现的神经网络" loading="lazy"></p>
<p>实现该神经网络。其中：</p>
<ul>
<li>输入层（0）2个神经元；</li>
<li>第一个隐藏层（1）3个神经元，第二个隐藏层（2）2个神经元；</li>
<li>输出层（3）2个神经元。</li>
</ul>
<p>书中给我们定义了符号来表示神经元和信号，以权重的符号为例：</p>
<p><img src="https://s2.loli.net/2022/10/06/p8sriFWC2tVNb1O.png" alt="权重的符号" loading="lazy"></p>
<h3 id="信号传递"><a href="#信号传递" class="headerlink" title="信号传递"></a>信号传递</h3><p>示例，从输入层到第1层的信号传递，考虑偏置：</p>
<p><img src="https://s2.loli.net/2022/10/06/GkWuzmDV1QCqM2E.png" alt="输入层到第1层的信号传递" loading="lazy"></p>
<p>可以用数学式来表示 $a_1^{(1)}$：</p>
<p>$$ a_1^{(1)} &#x3D; w_{1 1}^{(1)}x_1 + w_{1 2}^{(1)}x_2 + b_1^{(1)} $$</p>
<p>如果使用矩阵乘法，可以将第1层的加权和表示为如下形式：</p>
<p>$$ \boldsymbol{A}^{(1)} &#x3D; \boldsymbol{X}\boldsymbol{W}^{(1)} + \boldsymbol{B}^{(1)} $$</p>
<p>其中：</p>
<p>$$ \boldsymbol{A}^{(1)} &#x3D; \begin{pmatrix} a_1^{(1)} &amp; a_2^{(1)} &amp; a_3^{(1)} \end{pmatrix} \\ \boldsymbol{X} &#x3D; \begin{pmatrix} x_1 &amp; x_2 \end{pmatrix} \\ \boldsymbol{B}^{(1)} &#x3D; \begin{pmatrix} b_1^{(1)} &amp; b_2^{(1)} &amp; b_3^{(1)} \end{pmatrix} \\ \boldsymbol{W}^{(1)} &#x3D;  \begin{pmatrix} w_{1 1}^{(1)} &amp; w_{2 1}^{(1)} &amp; w_{3 1}^{(1)} \\ w_{1 2}^{(1)} &amp; w_{2 2}^{(1)} &amp; w_{3 2}^{(1)} \end{pmatrix} $$</p>
<p>写成 Python 实现（手动指定权重和偏置）：</p>
<pre class="language-python" data-language="python"><code class="language-python">X <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># shape: (2, 3)</span>
W1 <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.3</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.4</span><span class="token punctuation">,</span> <span class="token number">0.6</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># shape: (2,)</span>
B1 <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># shape: (3,)</span>
A1 <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>X<span class="token punctuation">,</span> W1<span class="token punctuation">)</span> <span class="token operator">+</span> B1</code></pre>

<p>考虑激活函数，那么第1层的计算过程就是：</p>
<p><img src="https://s2.loli.net/2022/10/06/T3uDapWMw1eK9Xq.png" alt="输入层到第1层的信号传递2" loading="lazy"></p>
<p>上图中隐藏层的加权和（加权信号与偏置的和）用 $a$ 表示，被激活函数转换后的信号用 $z$ 表示。$h()$ 表示激活函数，如果使用 sigmoid 函数作为激活函数，$z$ 的计算方法就是 <code>Z1 = sigmoid(A1)</code>。</p>
<p>用同样的方法可以实现第1层到第2层的信号传递：</p>
<p><img src="https://s2.loli.net/2022/10/06/Q8c1eOVod6WFSZu.png" alt="第1层到第2层的信号传递" loading="lazy"></p>
<pre class="language-python" data-language="python"><code class="language-python">W2 <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.4</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0.3</span><span class="token punctuation">,</span> <span class="token number">0.6</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
B2 <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
A2 <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>Z1<span class="token punctuation">,</span> W2<span class="token punctuation">)</span> <span class="token operator">+</span> B2  <span class="token comment"># 第1层的输出是第2层的输入</span>
Z2 <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>A2<span class="token punctuation">)</span></code></pre>

<p>从第2层到输出层的实现与上面也基本一致，将恒等函数作为激活函数（输出层的激活函数用 $\sigma()$ 表示）：</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">identity_function</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> x

W3 <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
B3 <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
A3 <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>Z2<span class="token punctuation">,</span> W3<span class="token punctuation">)</span> <span class="token operator">+</span> B3
Y <span class="token operator">=</span> identity_function<span class="token punctuation">(</span>A3<span class="token punctuation">)</span>  <span class="token comment"># 或者Y = A3</span></code></pre>

<p>整理上面的实现：</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># 把权重记为大写字母，其他如偏置和中间结果等记为小写字母</span>
<span class="token keyword">def</span> <span class="token function">init_network</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    network <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>
    network<span class="token punctuation">[</span><span class="token string">'W1'</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.3</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.4</span><span class="token punctuation">,</span> <span class="token number">0.6</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    network<span class="token punctuation">[</span><span class="token string">'b1'</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    network<span class="token punctuation">[</span><span class="token string">'W2'</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.4</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0.3</span><span class="token punctuation">,</span> <span class="token number">0.6</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    network<span class="token punctuation">[</span><span class="token string">'b2'</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    network<span class="token punctuation">[</span><span class="token string">'W3'</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    network<span class="token punctuation">[</span><span class="token string">'b3'</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> network

<span class="token comment"># 将输入信号转化为输出信号的过程（前向传播？）</span>
<span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>network<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    W1<span class="token punctuation">,</span> W2<span class="token punctuation">,</span> W3 <span class="token operator">=</span> network<span class="token punctuation">[</span><span class="token string">'W1'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> network<span class="token punctuation">[</span><span class="token string">'W2'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> network<span class="token punctuation">[</span><span class="token string">'W3'</span><span class="token punctuation">]</span>
    b1<span class="token punctuation">,</span> b2<span class="token punctuation">,</span> b3 <span class="token operator">=</span> network<span class="token punctuation">[</span><span class="token string">'b1'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> network<span class="token punctuation">[</span><span class="token string">'b2'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> network<span class="token punctuation">[</span><span class="token string">'b3'</span><span class="token punctuation">]</span>
    a1 <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> W1<span class="token punctuation">)</span> <span class="token operator">+</span> b1
    z1 <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>a1<span class="token punctuation">)</span>
    a2 <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>z1<span class="token punctuation">,</span> W2<span class="token punctuation">)</span> <span class="token operator">+</span> b2
    z2 <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>a2<span class="token punctuation">)</span>
    a3 <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>z2<span class="token punctuation">,</span> W3<span class="token punctuation">)</span> <span class="token operator">+</span> b3
    y <span class="token operator">=</span> identity_function<span class="token punctuation">(</span>a3<span class="token punctuation">)</span>
    <span class="token keyword">return</span> y

network <span class="token operator">=</span> init_network<span class="token punctuation">(</span><span class="token punctuation">)</span>
x <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> forward<span class="token punctuation">(</span>network<span class="token punctuation">,</span> x<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span>  <span class="token comment"># [0.31682708 0.69627909]</span></code></pre>

<h2 id="输出层的设计"><a href="#输出层的设计" class="headerlink" title="输出层的设计"></a>输出层的设计</h2><p>神经网络可以用在分类问题和回归问题上，要根据情况改变使用的激活函数，一般回归问题用恒等函数，分类问题用 softmax 函数。softmax 函数可以用如下式子表示：</p>
<p>$$ y_k &#x3D; \frac{exp(a_k)}{\sum\limits_{i&#x3D;1}^{n}exp(a_i)} $$</p>
<p>其中 $exp()$ 是表示 $e^x$ 的指数函数。该式子假设输出层共有 $n$ 个神经元，计算第 $k$ 个神经元的输出 $y_k$。softmax 函数的分子是输入信号 $a_k$ 的指数函数，分母是所有输入信号的指数函数的和。</p>
<p>从上面可以看出输出层的各个神经元都受到所有输入信号的影响。用图表示这个函数，它的输出与所有的输入信号相连：</p>
<p><img src="https://s2.loli.net/2022/10/06/hANMxfLWD6pBvl5.png" alt="softmax 函数" loading="lazy"></p>
<h3 id="softmax-函数"><a href="#softmax-函数" class="headerlink" title="softmax 函数"></a>softmax 函数</h3><p>直接实现 softmax 函数：</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">softmax</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">:</span>
    exp_a <span class="token operator">=</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>a<span class="token punctuation">)</span>
    sum_exp_a <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>exp_a<span class="token punctuation">)</span>
    y <span class="token operator">=</span> exp_a <span class="token operator">/</span> sum_exp_a
    <span class="token keyword">return</span> y</code></pre>

<p>这个实现有缺陷，主要问题出在 <code>exp()</code> 上。<code>exp(x)</code> 表示 $e^x$，当 x 比较大的时候，结果很容易溢出（可以自行算一下，<code>int</code> 类型根本不够用）。书的作者介绍了一种常用的优化方法，首先看这个式子：</p>
<p>$$ \begin{aligned} y_k &amp;&#x3D; \frac{exp(a_k)}{\sum \limits_{i &#x3D; 1}^n exp(a_i)} \\ &amp;&#x3D; \frac{C exp(a_k)}{C \sum \limits_{i &#x3D; 1}^n exp(a_i)} \\ &amp;&#x3D; \frac{exp(a_k + \log C)}{\sum \limits_{i &#x3D; 1}^n exp(a_i + \log C)} \\ &amp;&#x3D; \frac{exp(a_k + C’)}{\sum \limits_{i &#x3D; 1}^n exp(a_i + C’)} \end{aligned} $$</p>
<p>从这个式子可以看出，计算时在指数上加减任意常数都不会改变运算结果。为了防止溢出，一般是减去输入信号中的最大值。作者给出的例子：</p>
<pre class="language-pycon" data-language="pycon"><code class="language-pycon">&gt;&gt;&gt; a &#x3D; np.array([1010, 1000, 990])
&gt;&gt;&gt; np.exp(a) &#x2F; np.sum(np.exp(a))  # softmax函数的运算
array([ nan, nan, nan])  # 溢出
&gt;&gt;&gt;
&gt;&gt;&gt; c &#x3D; np.max(a) # 1010
&gt;&gt;&gt; a - c
array([ 0, -10, -20])
&gt;&gt;&gt;
&gt;&gt;&gt; np.exp(a - c) &#x2F; np.sum(np.exp(a - c))
array([ 9.99954600e-01, 4.53978686e-05, 2.06106005e-09])</code></pre>

<p>所以 softmax 函数的实现就可以改成：</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">softmax</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">:</span>
    c <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span>  <span class="token comment"># 输入信号的最大值</span>
    exp_a <span class="token operator">=</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>a <span class="token operator">-</span> c<span class="token punctuation">)</span>  <span class="token comment"># 溢出对策</span>
    sum_exp_a <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>exp_a<span class="token punctuation">)</span>
    y <span class="token operator">=</span> exp_a <span class="token operator">/</span> sum_exp_a
    <span class="token keyword">return</span> y</code></pre>

<p>softmax 函数的输出是0.0到1.0之间的实数，输出值的总和是1，所以 softmax 函数的输出也可以被解释为概，也因为这样，softmax 函数适用于分类问题。</p>
<p>由于 softmax 函数不会改变各个元素之间的大小关系（因为指数函数是单调递增的），如果只需要知道输出值最大的神经元对应的类别，可以忽略输出层的 softmax 函数以减少运算量。</p>
<h3 id="输出层的神经元数量"><a href="#输出层的神经元数量" class="headerlink" title="输出层的神经元数量"></a>输出层的神经元数量</h3><p>输出层的神经元数量取决于待解决的问题。对于分类问题，一般把输出层神经元数量设定为类别的数量。</p>
<h2 id="实际问题：手写数字识别"><a href="#实际问题：手写数字识别" class="headerlink" title="实际问题：手写数字识别"></a>实际问题：手写数字识别</h2><p>对于这个问题，书的作者提供了可以直接使用的学习结果，只需要实现神经网络的推理处理过程（前向传播）就可以了。</p>
<blockquote>
<p>使用 MNIST 数据集。该数据集由0到9的数字图像构成，训练图像6万张，测试图像1万张。先用训练图像进行学习，再用学习到的模型度量能在多大程度上对测试图像进行正确的分类。</p>
<p>MNIST的图像数据是28像素 × 28像素的灰度图像（1通道），各个像素的取值在0到255之间。每个图像数据都相应地标有“7”、“2”、“1”等标签。</p>
</blockquote>
<p>下载、加载数据集等需要提前准备的代码作者也给出来了，在<a target="_blank" rel="noopener" href="https://github.com/oreilly-japan/deep-learning-from-scratch/blob/master/dataset/mnist.py">这里</a>。</p>
<p>需要实现的神经网络输入层有784个神经元（每张图片784像素），输出层有10个神经元（10个数字分类）。按照作者的提示，设置两个隐藏层，第一个隐藏层有50个神经元，第二个隐藏层有100个神经元。</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> sys<span class="token punctuation">,</span> os
sys<span class="token punctuation">.</span>path<span class="token punctuation">.</span>append<span class="token punctuation">(</span>os<span class="token punctuation">.</span>curdir<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>sys<span class="token punctuation">.</span>path<span class="token punctuation">)</span>
<span class="token keyword">from</span> dataset<span class="token punctuation">.</span>mnist <span class="token keyword">import</span> load_mnist
<span class="token keyword">import</span> pickle
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

<span class="token comment"># 实现 sigmoid 函数</span>
<span class="token comment"># 当然像这种通用的函数最好特意找个地方写实现，不要写得到处都是……</span>
<span class="token keyword">def</span> <span class="token function">sigmoid</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token number">1</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 由于 numpy 的广播功能，该实现可以支持 numpy 数组</span>

<span class="token keyword">def</span> <span class="token function">softmax</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">:</span>
    c <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span>
    exp_a <span class="token operator">=</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>a <span class="token operator">-</span> c<span class="token punctuation">)</span> <span class="token comment"># 溢出对策</span>
    sum_exp_a <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>exp_a<span class="token punctuation">)</span>
    y <span class="token operator">=</span> exp_a <span class="token operator">/</span> sum_exp_a
    <span class="token keyword">return</span> y

<span class="token comment"># 获取测试数据</span>
<span class="token comment"># 对数据进行了正规化处理（属于预处理），使数据的值在0.0～1.0的范围内</span>
<span class="token keyword">def</span> <span class="token function">get_data</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> t_train<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> t_test<span class="token punctuation">)</span> <span class="token operator">=</span> load_mnist<span class="token punctuation">(</span>normalize<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> flatten<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> one_hot_label<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> x_test<span class="token punctuation">,</span> t_test

<span class="token comment"># 读入学习到的权重参数</span>
<span class="token keyword">def</span> <span class="token function">init_network</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># rb, read as text, binary</span>
    <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"ch0203/sample_weight.pkl"</span><span class="token punctuation">,</span> <span class="token string">'rb'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
        network <span class="token operator">=</span> pickle<span class="token punctuation">.</span>load<span class="token punctuation">(</span>f<span class="token punctuation">)</span>
    <span class="token keyword">return</span> network

<span class="token comment"># 前向传播推理过程</span>
<span class="token keyword">def</span> <span class="token function">predict</span><span class="token punctuation">(</span>network<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    w1<span class="token punctuation">,</span> w2<span class="token punctuation">,</span> w3 <span class="token operator">=</span> network<span class="token punctuation">[</span><span class="token string">'W1'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> network<span class="token punctuation">[</span><span class="token string">'W2'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> network<span class="token punctuation">[</span><span class="token string">'W3'</span><span class="token punctuation">]</span>
    b1<span class="token punctuation">,</span> b2<span class="token punctuation">,</span> b3 <span class="token operator">=</span> network<span class="token punctuation">[</span><span class="token string">'b1'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> network<span class="token punctuation">[</span><span class="token string">'b2'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> network<span class="token punctuation">[</span><span class="token string">'b3'</span><span class="token punctuation">]</span>

    a1 <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> w1<span class="token punctuation">)</span> <span class="token operator">+</span> b1
    z1 <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>a1<span class="token punctuation">)</span>
    a2 <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>z1<span class="token punctuation">,</span> w2<span class="token punctuation">)</span> <span class="token operator">+</span> b2
    z2 <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>a2<span class="token punctuation">)</span>
    a3 <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>z2<span class="token punctuation">,</span> w3<span class="token punctuation">)</span> <span class="token operator">+</span> b3
    y <span class="token operator">=</span> softmax<span class="token punctuation">(</span>a3<span class="token punctuation">)</span>
    <span class="token keyword">return</span> y

x<span class="token punctuation">,</span> t <span class="token operator">=</span> get_data<span class="token punctuation">(</span><span class="token punctuation">)</span>
network <span class="token operator">=</span> init_network<span class="token punctuation">(</span><span class="token punctuation">)</span>
accuracy_cnt <span class="token operator">=</span> <span class="token number">0</span>  <span class="token comment"># 识别精度</span>
<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    y <span class="token operator">=</span> predict<span class="token punctuation">(</span>network<span class="token punctuation">,</span> x<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>
    p <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>y<span class="token punctuation">)</span>  <span class="token comment"># 取概率最高的元素的索引</span>
    <span class="token keyword">if</span> p <span class="token operator">==</span> t<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">:</span>
        accuracy_cnt <span class="token operator">+=</span> <span class="token number">1</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Accuracy: "</span> <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span><span class="token builtin">float</span><span class="token punctuation">(</span>accuracy_cnt<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre>

<p>PS：如果去看作者给的<a target="_blank" rel="noopener" href="https://github.com/oreilly-japan/deep-learning-from-scratch/blob/master/ch03/neuralnet_mnist.py">实现代码</a>，会发现最开始添加路径，作者添加了 <code>pardir</code>，我这里添加的是 <code>curdir</code>，读取权重时的文件路径也不太一样。这是因为我打开项目时的工作目录在 <code>ch0203</code>（相当于作者的 <code>ch03</code>）上面一层，而作者设定的工作目录就是 <code>ch03</code>……嗯，应该是这个原因吧，我还搜了挺长一段时间……😢</p>
<h3 id="批处理"><a href="#批处理" class="headerlink" title="批处理"></a>批处理</h3><p>观察一下神经网络各层权重的形状：</p>
<pre class="language-pycon" data-language="pycon"><code class="language-pycon">&gt;&gt;&gt; x, _ &#x3D; get_data()
&gt;&gt;&gt; network &#x3D; init_network()
&gt;&gt;&gt; W1, W2, W3 &#x3D; network[&#39;W1&#39;], network[&#39;W2&#39;], network[&#39;W3&#39;]
&gt;&gt;&gt;
&gt;&gt;&gt; x.shape
(10000, 784)
&gt;&gt;&gt; x[0].shape
(784,)
&gt;&gt;&gt; W1.shape
(784, 50)
&gt;&gt;&gt; W2.shape
(50, 100)
&gt;&gt;&gt; W3.shape
(100, 10)</code></pre>

<p><del>显然符合矩阵运算的要求。</del>多维数组形状变化过程如图（$X$ 的形状也可以表示为 $1 \times 784$，同理 $Y$ 的形状也可以表示为 $1 \times 10$）：</p>
<p><img src="https://s2.loli.net/2022/10/07/l25NM8BjZ3ohcfg.png" alt="数组形状的变化" loading="lazy"></p>
<p>从整体的流程来看，输入一个由784个元素构成的一维数组后，输出一个有10个元素的一维数组。考虑一次性处理100张图片，那么输入数组的形状可以改成 $100 \times 784$，此时多维数组&#x2F;矩阵形状的变化过程就会是：</p>
<p><img src="https://s2.loli.net/2022/10/07/3eWDaKFkYHp6riU.png" alt="批处理中数组形状的变化" loading="lazy"></p>
<p>这样输出数据的形状就是 $100 \times 10$。<em>这表示输入的100张图像的结果被一次性输出了。比如，<code>x[0]</code>和<code>y[0]</code>中保存了第0张图像及其推理结果，x[1]和y[1]中保存了第1张图像及其推理结果，等等。</em> </p>
<p><em>这种打包式的输入数据被称为批（batch）。</em></p>
<p>批处理的实现：</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> sys<span class="token punctuation">,</span> os
sys<span class="token punctuation">.</span>path<span class="token punctuation">.</span>append<span class="token punctuation">(</span>os<span class="token punctuation">.</span>curdir<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>sys<span class="token punctuation">.</span>path<span class="token punctuation">)</span>
<span class="token keyword">from</span> dataset<span class="token punctuation">.</span>mnist <span class="token keyword">import</span> load_mnist
<span class="token keyword">import</span> pickle
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

<span class="token comment"># 实现 sigmoid 函数</span>
<span class="token comment"># 当然像这种通用的函数最好特意找个地方写实现，不要写得到处都是……</span>
<span class="token keyword">def</span> <span class="token function">sigmoid</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token number">1</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 由于 numpy 的广播功能，该实现可以支持 numpy 数组</span>

<span class="token keyword">def</span> <span class="token function">softmax</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">:</span>
    c <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span>
    exp_a <span class="token operator">=</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>a <span class="token operator">-</span> c<span class="token punctuation">)</span> <span class="token comment"># 溢出对策</span>
    sum_exp_a <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>exp_a<span class="token punctuation">)</span>
    y <span class="token operator">=</span> exp_a <span class="token operator">/</span> sum_exp_a
    <span class="token keyword">return</span> y

<span class="token comment"># 获取测试数据</span>
<span class="token comment"># 对数据进行了正规化处理（属于预处理），使数据的值在0.0～1.0的范围内</span>
<span class="token keyword">def</span> <span class="token function">get_data</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> t_train<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> t_test<span class="token punctuation">)</span> <span class="token operator">=</span> load_mnist<span class="token punctuation">(</span>normalize<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> flatten<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> one_hot_label<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> x_test<span class="token punctuation">,</span> t_test

<span class="token comment"># 读入学习到的权重参数</span>
<span class="token keyword">def</span> <span class="token function">init_network</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># rb, read as text, binary</span>
    <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"ch0203/sample_weight.pkl"</span><span class="token punctuation">,</span> <span class="token string">'rb'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
        network <span class="token operator">=</span> pickle<span class="token punctuation">.</span>load<span class="token punctuation">(</span>f<span class="token punctuation">)</span>
    <span class="token keyword">return</span> network

<span class="token comment"># 前向传播推理过程</span>
<span class="token keyword">def</span> <span class="token function">predict</span><span class="token punctuation">(</span>network<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    w1<span class="token punctuation">,</span> w2<span class="token punctuation">,</span> w3 <span class="token operator">=</span> network<span class="token punctuation">[</span><span class="token string">'W1'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> network<span class="token punctuation">[</span><span class="token string">'W2'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> network<span class="token punctuation">[</span><span class="token string">'W3'</span><span class="token punctuation">]</span>
    b1<span class="token punctuation">,</span> b2<span class="token punctuation">,</span> b3 <span class="token operator">=</span> network<span class="token punctuation">[</span><span class="token string">'b1'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> network<span class="token punctuation">[</span><span class="token string">'b2'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> network<span class="token punctuation">[</span><span class="token string">'b3'</span><span class="token punctuation">]</span>

    a1 <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> w1<span class="token punctuation">)</span> <span class="token operator">+</span> b1
    z1 <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>a1<span class="token punctuation">)</span>
    a2 <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>z1<span class="token punctuation">,</span> w2<span class="token punctuation">)</span> <span class="token operator">+</span> b2
    z2 <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>a2<span class="token punctuation">)</span>
    a3 <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>z2<span class="token punctuation">,</span> w3<span class="token punctuation">)</span> <span class="token operator">+</span> b3
    y <span class="token operator">=</span> softmax<span class="token punctuation">(</span>a3<span class="token punctuation">)</span>
    <span class="token keyword">return</span> y

x<span class="token punctuation">,</span> t <span class="token operator">=</span> get_data<span class="token punctuation">(</span><span class="token punctuation">)</span>
network <span class="token operator">=</span> init_network<span class="token punctuation">(</span><span class="token punctuation">)</span>

batch_size <span class="token operator">=</span> <span class="token number">100</span>  <span class="token comment"># “一批”的大小</span>
accuracy_cnt <span class="token operator">=</span> <span class="token number">0</span>  <span class="token comment"># 识别精度</span>

<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># range(start, end, step)</span>
    x_batch <span class="token operator">=</span> x<span class="token punctuation">[</span>i<span class="token punctuation">:</span>i<span class="token operator">+</span>batch_size<span class="token punctuation">]</span>  <span class="token comment"># 按批取出数据</span>
    y_batch <span class="token operator">=</span> predict<span class="token punctuation">(</span>network<span class="token punctuation">,</span> x_batch<span class="token punctuation">)</span>
    p <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>y_batch<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 在100*10的数组中，沿着第1维（从0开始数）方向找到值最大的元素的索引</span>
    accuracy_cnt <span class="token operator">+=</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>p <span class="token operator">==</span> t<span class="token punctuation">[</span>i<span class="token punctuation">:</span>i<span class="token operator">+</span>batch_size<span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 使用比较运算符会生成布尔值数组，sum 会计算 True 的个数</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Accuracy: "</span> <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span><span class="token builtin">float</span><span class="token punctuation">(</span>accuracy_cnt<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre>

<p><em>大多数处理数值计算的库都进行了能够高效处理大型数组运算的最优化</em>，而且批处理可以减少读取数据方面的开销，所以批处理计算大型数组比分开计算各个小数组速度快。</p>
<p>PPS：这一部分说是神经网络，其实应该说是神经网络的前向传播咯……那就先到这里。</p>
</div></section><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>零歌</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="https://afool.top/learning/deep-learning-from-scratch-neural-network/" title="深度学习入门：神经网络">https://afool.top/learning/deep-learning-from-scratch-neural-network/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><span class="icon iconify" data-icon="ri:creative-commons-line"></span><span class="icon iconify" data-icon="ri:creative-commons-by-line"></span><span class="icon iconify" data-icon="ri:creative-commons-nc-line"></span><span class="icon iconify" data-icon="ri:creative-commons-sa-line"></span></a> 许可协议。</li></ul></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/learning/deep-learning-from-scratch-learning-of-neural-network/" rel="prev" title="深度学习入门：神经网络的学习"><span class="icon iconify" data-icon="ri:arrow-left-s-line"></span><span class="post-nav-text">深度学习入门：神经网络的学习</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/learning/deep-learning-from-scratch-perception/" rel="next" title="深度学习入门：感知机"><span class="post-nav-text">深度学习入门：感知机</span><span class="icon iconify" data-icon="ri:arrow-right-s-line"></span></a></div></div></div><div class="hty-card" id="comment"><div class="comment-tooltip text-center"><span>留个爪印吧~</span><br></div><div id="tcomment"></div><script type="module">import { getScript } from '/js/utils.js'

getScript("https://fastly.jsdelivr.net/npm/twikoo@latest/dist/twikoo.all.min.js", () => {
  const twikooConfig = {"enable":true,"envId":"https://twikoo-afool.vercel.app/"}
  twikooConfig.el = '#tcomment'
  twikoo.init(twikooConfig)
}, window.twikoo);</script></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2020 – 2022 </span><span class="with-love" id="animate"><span class="icon iconify" data-icon="ri:cloud-line"></span></span><span class="author"> 零歌</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v6.3.0</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.10.9</span></div></footer></div><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><span class="icon iconify" data-icon="ri:arrow-up-s-line"></span><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a></body></html>